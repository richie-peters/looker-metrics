
PROJECT CODE COMPILATION
Generated: Wed Jul  2 07:26:54 AM UTC 2025
Directory: /content/looker-metrics
Files processed: 6

================================================================================
TABLE OF CONTENTS
================================================================================

================================================================================
FILE: config.py
SIZE: 5833 characters
================================================================================

"""
Configuration and Setup Module (EMAIL_LIST FIXED)
=================================================

This module handles environment setup, authentication, and project configurations.

Author: Data Team
Date: 2025-01-07
"""

import pandas as pd
import os
import sys
from datetime import datetime
import warnings
from google.cloud import bigquery, storage
import vertexai
from google.auth import default

# Project-specific configurations
BQ_PROJECT_ID = "ncau-data-newsquery-prd"
VERTEX_PROJECT_ID = "ncau-data-nprod-aitrain"
REGION = "us-central1"

# GCS Paths
INPUT_GCS_URI = "gs://looker_metrics/input.jsonl"
OUTPUT_GCS_URI = "gs://looker_metrics/output/"

# Gemini Model Configuration
GEMINI_MODEL_NAME = "gemini-2.5-flash"
MAX_OUTPUT_TOKENS = 65000
TEMPERATURE = 0.20

# SQL File Path
LOOKER_SQL_FILE = "./sql/looker_sql.txt"

# Email List for SQL Queries
EMAIL_LIST = [
    'santhosh.kanaparthi@news.com.au','desnica.kumar@news.com.au','romy.li@news.com.au','cecile.desphy@news.com.au','jeyaram.jawahar@news.com.au','nigel.aye@news.com.au','camille.shi@news.com.au','justin.guo@news.com.au','eric.loi@news.com.au','pravarthika.rathinakumar@news.com.au','kylie.lu@news.com.au','ritwik.deo@news.com.au','harry.mcauley@news.com.au'
]

# Directory Structure
BASE_PATH = "."
DIRECTORIES = {
    'functions': os.path.join(BASE_PATH, 'functions'),
    'scripts': os.path.join(BASE_PATH, 'scripts'),
    'data': os.path.join(BASE_PATH, 'data'),
    'sql': os.path.join(BASE_PATH, 'sql'),
    'results': os.path.join(BASE_PATH, 'results'),
    'logs': os.path.join(BASE_PATH, 'logs')
}

def setup_environment():
    """Configure global environment settings for analysis."""
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 50)
    pd.set_option('display.float_format', lambda x: '%.2f' % x)
    warnings.filterwarnings('ignore')
    print("✓ Environment configured successfully")

def create_directory_structure(base_path=BASE_PATH):
    """Create standard directory structure for the project."""
    for name, path in DIRECTORIES.items():
        os.makedirs(path, exist_ok=True)
        print(f"✓ Directory ready: {name} -> {path}")
    return DIRECTORIES

def validate_project_setup(bq_project_id=BQ_PROJECT_ID, vertex_project_id=VERTEX_PROJECT_ID, region=REGION):
    """Validate that GCP projects and services are properly configured."""
    validation_results = {
        'bigquery': False,
        'vertex_ai': False,
        'storage': False,
        'authentication': False,
        'projects_valid': False
    }

    try:
        bq_client = bigquery.Client(project=bq_project_id)
        bq_client.query("SELECT 1 as test").result()
        validation_results['bigquery'] = True
        print(f"✓ BigQuery access validated: {bq_project_id}")
    except Exception as e:
        print(f"✗ BigQuery validation failed: {str(e)}")

    try:
        vertexai.init(project=vertex_project_id, location=region)
        from vertexai.preview import generative_models
        model = generative_models.GenerativeModel("gemini-pro")
        validation_results['vertex_ai'] = True
        print(f"✓ Vertex AI access validated: {vertex_project_id}")
    except Exception as e:
        print(f"✗ Vertex AI validation failed: {str(e)}")

    try:
        storage_client = storage.Client()
        validation_results['storage'] = True
        print(f"✓ Cloud Storage access validated")
    except Exception as e:
        print(f"✗ Cloud Storage validation failed: {str(e)}")

    try:
        credentials, project = default()
        validation_results['authentication'] = True
        validation_results['default_project'] = project
        print(f"✓ Authentication validated: {project}")
    except Exception as e:
        print(f"✗ Authentication validation failed: {str(e)}")

    validation_results['projects_valid'] = all(validation_results[key] for key in ['bigquery', 'vertex_ai', 'storage', 'authentication'])

    if validation_results['projects_valid']:
        print(f"✓ All project validations passed")
    else:
        print(f"✗ Some validations failed - check configuration")

    return validation_results

def initialize_session():
    """Initialise a complete analysis session."""
    print("=" * 60)
    print("INITIALIZING LOOKER ANALYSIS SESSION")
    print("=" * 60)

    setup_environment()
    directories = create_directory_structure()
    validation_results = validate_project_setup()

    session_config = {
        'session_id': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'start_time': datetime.now(),
        'directories': directories,
        'project_config': {
            'bq_project': BQ_PROJECT_ID,
            'vertex_project': VERTEX_PROJECT_ID,
            'region': REGION
        },
        'validation_results': validation_results
    }

    print("\n" + "=" * 60)
    print("✓ ANALYSIS SESSION INITIALIZED")
    print("=" * 60)
    print(f"Session ID: {session_config['session_id']}")
    print(f"Start time: {session_config['start_time']}")
    print(f"BigQuery Project: {session_config['project_config']['bq_project']}")
    print(f"Vertex AI Project: {session_config['project_config']['vertex_project']}")

    return session_config



def print_session_summary(session_config):
    """Print a summary of the current session configuration."""
    print("\n" + "=" * 60)
    print("SESSION SUMMARY")
    print("=" * 60)

    print(f"BigQuery Project: {session_config['project_config']['bq_project']}")
    print(f"Vertex AI Project: {session_config['project_config']['vertex_project']}")
    print(f"Region: {session_config['project_config']['region']}")
    print(f"Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Working Directory: {os.getcwd()}")
    print("\n" + "=" * 60)


================================================================================
END OF FILE: config.py
================================================================================


================================================================================
FILE: README.md
SIZE: 3251 characters
================================================================================

# Looker Studio Analysis Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-1e3a5f?style=for-the-badge&logo=python&logoColor=f5f5dc)
![Google Cloud](https://img.shields.io/badge/Google_Cloud-BigQuery_|_Vertex_AI-1e3a5f?style=for-the-badge&logo=google-cloud&logoColor=f5f5dc)
![Colab](https://img.shields.io/badge/Google-Colab-1e3a5f?style=for-the-badge&logo=google-colab&logoColor=f5f5dc)

**Automated pipeline for analysing Looker Studio metrics using BigQuery and Gemini AI**

</div>

<br>

## Overview

Enterprise-grade pipeline that extracts data from BigQuery and processes it using Gemini AI for intelligent Looker Studio analytics.

<br>

## Features

- **BigQuery Integration** - Automated data extraction with SQL templates
- **AI Processing** - Gemini-powered analysis and insights  
- **Batch Operations** - Scalable processing for large datasets
- **CSV Export** - Structured output for further analysis

<br>

## Installation & Execution

### Step 1: Setup Repository
```python
# Clone and navigate to project
!git clone https://github.com/richie-peters/looker-metrics.git
import os
os.chdir('/content/looker-metrics')
Step 2: Install Dependencies# Install required packages
!pip install -q -r requirements.txt
Step 3: Authenticate# Google Cloud authentication
from google.colab import auth
auth.authenticate_user()
Step 4: ConfigureUpdate config.py with your settings:
BQ_PROJECT_ID - Your BigQuery project
VERTEX_PROJECT - Your Vertex AI project
INPUT_GCS_URI - Input bucket path
OUTPUT_GCS_URI - Output bucket path
Step 5: Run Pipeline# Execute the analysis
exec(open('main.py').read())
<br>ConfigurationParameterDescriptionExampleBQ_PROJECT_IDBigQuery project ID"my-project"VERTEX_PROJECTVertex AI project ID"my-vertex-project"INPUT_GCS_URIInput storage location"gs://bucket/input/"OUTPUT_GCS_URIOutput storage location"gs://bucket/output/"EMAIL_LISTTarget emails for analysisList of email strings<br>Project Structurelooker-metrics/
├── main.py              # Main execution script
├── config.py            # Configuration settings  
├── functions.py         # Core functions
├── sql/                 # SQL query templates
├── data/                # Output directory
└── requirements.txt     # Dependencies
<br>TroubleshootingAuthentication Issuesfrom google.colab import auth
auth.authenticate_user()
No Data Returned
Check BigQuery permissions
Verify email list configuration
Confirm dataset exists
Processing Errors
Enable Vertex AI API
Validate GCS bucket access
Check model availability
Debug Results# Check processing output
if 'global_results' in globals():
    print(f"Processed: {len(global_results)} records")
<br>Output
CSV Files - Generated in ./data/ directory
Console Logs - Execution summary and statistics
Global Variables - Results stored in global_results
<br>CustomisationEmail List: Edit EMAIL_LIST in main.pyEMAIL_LIST = ['user@company.com', 'user2@company.com']
Processing: Modify parameters in config.py and functions.py<br>Support
Review documentation and troubleshooting section
Check execution logs for error details
Contact Data Team for assistance
Submit GitHub issues for bugs
<br><div align="center">Enterprise Data Pipeline SolutionData Team | Production Ready</div>
```

================================================================================
END OF FILE: README.md
================================================================================


================================================================================
FILE: main.py
SIZE: 1965 characters
================================================================================

"""
Looker Studio Analysis Pipeline - Direct Execution
==================================================
"""

import pandas as pd
import config
import functions

# Configuration
EMAIL_LIST = ["richie.peters@news.com.au"]
global_results = None

# 1. Initialise session
session_config = config.initialize_session()
config.print_session_summary(session_config)

# 2. Extract data from BigQuery
print("\n--- Extracting data from BigQuery ---")
replacements = {'INSERT': functions.format_emails_for_sql(EMAIL_LIST)}
query_df = functions.run_sql_file(config.LOOKER_SQL_FILE, replacements=replacements, project=config.BQ_PROJECT_ID)

if query_df is None or query_df.empty:
    print("✗ No data extracted from BigQuery. Exiting.")
else:
    # 3. Prepare batch input for Gemini
    print("\n--- Preparing batch input for Gemini ---")
    batch_data = functions.prepare_looker_analysis_batch(query_df)

    # 4. Run Gemini batch prediction (SLICK VERSION)
    print("\n--- Running Gemini batch prediction ---")
    results = functions.run_gemini_batch_fast_slick(
        requests=batch_data,
        project=session_config['project_config']['vertex_project'],
        display_name="looker-analysis-batch",
        input_gcs_uri=config.INPUT_GCS_URI,
        output_gcs_uri=config.OUTPUT_GCS_URI,
        model_name=config.GEMINI_MODEL_NAME
    )

    if results is None:
        print("✗ Gemini batch prediction failed. Exiting.")
    else:
        # Make results global for troubleshooting
        global_results = results
        print(f"✓ Results stored in global_results variable ({len(results)} items)")

        # 5. Process and save results
        print("\n--- Processing results ---")
        datasets = functions.convert_batch_results_to_dataset(results)
        if datasets:
            functions.save_datasets_to_csv(datasets, "./data/")
            functions.analyse_results_summary(datasets)

        print(f"✓ Successfully processed {len(results)} results.")

================================================================================
END OF FILE: main.py
================================================================================


================================================================================
FILE: functions.py
SIZE: 97709 characters
================================================================================

"""
Consolidated Functions Module (With Looker Analysis Prompt)
===========================================================

This module contains all custom functions for the Looker Studio analysis pipeline,
including SQL utilities, batch processing utilities, and data processing utilities.

Author: Data Team
Date: 2025-01-07
"""

import json
import time
import os
import sys
from datetime import datetime
from google.cloud import bigquery, storage
from google.cloud import aiplatform_v1beta1, aiplatform_v1
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
import vertexai
from vertexai.generative_models import GenerativeModel
import pandas as pd
import numpy as np

# Looker Analysis Prompt
LOOKER_ANALYSIS_PROMPT = """
Analyze these Looker Studio dashboard SQL queries and extract comprehensive metrics information with unified dataset analysis.

INPUT DATA:
- Dashboard ID: {dashboard_id}
- Dashboard Name: {dashboard_name}
- SQL Samples: {sql_samples}

ANALYSIS REQUIREMENTS:

1. **METRIC EXTRACTION**: Identify all business metrics, dimensions, and calculations
2. **SQL DECOMPOSITION**: Break down complex nested queries into logical components
3. **DEPENDENCY MAPPING**: Identify which calculations depend on others
4. **BUSINESS LOGIC**: Extract the core business rules and transformations
5. **DATASET STRUCTURE**: Understand data grain, key dimensions, and metric interactions
6. **HARDCODED VALUES**: Identify hardcoded dates, values, and variables that should be parameterised or use governed tables
7. **UNIFIED ANALYSIS**: Create consolidated queries that analyze all metrics together
8. **STANDARDISATION**: Use standardised classes and values for consistent analysis
9. **BIGQUERY COMPLIANCE**: Generate only valid BigQuery SQL syntax

OUTPUT REQUIREMENTS (JSON):
Return a flat JSON structure with the following schema:

{{
  "dashboard_summary": {{
    "dashboard_id": "string",
    "dashboard_name": "string",
    "primary_data_sources": "project1.dataset1.table1;project2.dataset2.table2;project3.dataset3.table3",
    "business_domain": "advertising|finance|consumer|operations|marketing|sales|product|hr|other",
    "complexity_score": 1-10,
    "consolidation_score": 1-10,
    "total_metrics_identified": number,
    "date_grain": "daily|weekly|monthly|quarterly|yearly|mixed|none",
    "data_grain": "transactional|aggregate",
    "key_dimensions": ["date", "customer_id", "product", "region"],
    "date_range_detected": "01/01/2024 to Current",
    "hardcoded_dates_found": ["03/07/2024", "30/03/2025", "01/01/2024"],
    "hardcoded_values_found": ["specific product IDs", "region codes", "business unit names"],
    "governance_opportunities": ["dates should be parameterised", "product codes should join to product_master", "regions should use location_hierarchy"]
  }},
  "dataset_analysis": {{
    "primary_analysis_sql": "**THIS IS THE MAIN SQL TO RUN** - Single query showing all key metrics calculated together with appropriate sampling and date filters",
    "structure_sql": "Query to understand data structure, grain, and key dimensions with sampling",
    "validation_sql": "Quick validation that all metric calculations work syntactically",
    "business_rules_sql": "Query to validate key business logic, filters, and data quality",
    "sample_data_sql": "Query to get representative sample data for further analysis",
    "hardcoded_issues": {{
      "hardcoded_dates": [
        {{
          "date_value": "03/07/2024",
          "original_format": "2024-07-03T00:00:00",
          "context": "used as baseline date in DATETIME_DIFF calculation",
          "suggested_fix": "replace with CURRENT_DATE() or parameter",
          "impact": "high|medium|low",
          "urgency": "high|medium|low"
        }}
      ],
      "hardcoded_variables": [
        {{
          "variable_type": "lookup_codes|business_rules|thresholds|categories|other",
          "hardcoded_values": ["'TA'", "'DT'", "'HS'"],
          "context": "masthead codes hardcoded in CASE statement",
          "suggested_governance": "join to masthead_lookup table",
          "impact": "high|medium|low",
          "maintenance_risk": "high|medium|low"
        }}
      ]
    }},
    "parameterisation_recommendations": [
      "Replace hardcoded dates with date parameters or relative date functions",
      "Replace hardcoded lookup values with joins to governed reference tables",
      "Use configuration tables for business rules instead of hardcoded logic"
    ]
  }},
  "metrics": [
    {{
      "metric_id": "unique_identifier_snake_case",
      "metric_name": "Human Readable Name",
      "metric_type": "dimension|measure|calculated_field|filter|aggregation|ratio|percentage",
      "calculation_type": "sum|count|count_distinct|average|min|max|ratio|case_when|date_function|string_function|mathematical|conditional",
      "data_type": "numeric|string|date|boolean|array",
      "aggregation_level": "transaction|daily|weekly|monthly|quarterly|yearly|customer|product|region|custom",
      "is_final_output": true|false,
      "is_kpi": true|false,
      "business_criticality": "high|medium|low",
      "depends_on_metrics": ["metric_id1", "metric_id2"],
      "business_description": "what this metric represents in business terms",
      "sql_logic": "core SQL calculation logic extracted from queries",
      "data_sources": ["project.dataset.table1", "project.dataset.table2"],
      "filters_applied": ["date filters", "business rules", "exclusions"],
      "expected_data_type": "integer|decimal|string|date|boolean",
      "business_context": "how this metric fits into overall business analysis",
      "metric_category": "revenue|cost|volume|efficiency|quality|growth|retention|acquisition|other",
      "update_frequency": "real_time|hourly|daily|weekly|monthly|quarterly|yearly|on_demand",
      "seasonality_impact": "high|medium|low|none",
      "hardcoded_dates_in_metric": ["03/07/2024", "01/01/2025"],
      "hardcoded_values_in_metric": ["'Metro'", "'Regional'", "'TA'", "'DT'"],
      "governance_issues": ["date should be parameterised", "lookup values should use reference table"],
      "data_quality_concerns": ["potential nulls", "outliers expected", "data freshness dependent"]
    }}
  ],
  "metric_interactions": [
    {{
      "interaction_type": "mathematical_relationship|dependency|filter_impact|hierarchical|causal",
      "primary_metric": "metric_id",
      "related_metrics": ["metric_id1", "metric_id2"],
      "relationship_description": "how these metrics relate to each other",
      "mathematical_formula": "if applicable: A = B * C or A = B + C",
      "business_validation": "what this relationship means for business analysis",
      "validation_sql": "SQL to test this relationship holds true",
      "relationship_strength": "strong|medium|weak",
      "business_impact": "high|medium|low"
    }}
  ]
}}

CRITICAL BIGQUERY SQL REQUIREMENTS:

**NEVER DO THESE - THEY CAUSE FAILURES:**
❌ Compare INT64 with STRING: WHERE fiscal_week_id = 'CP'
❌ Use LPAD on INT64: LPAD(week_number, 2, '0')
❌ Create arrays with NULLs: ARRAY[col1, col2, null_col]
❌ Missing GROUP BY: SELECT customer, revenue FROM table GROUP BY customer
❌ Compare different types: WHERE year_column = 'CP' (if year_column is INT64)

**ALWAYS DO THESE - THEY WORK:**
✅ Cast before comparing: WHERE CAST(fiscal_week_id AS STRING) = 'CP' OR WHERE fiscal_week_id = CAST('2024' AS INT64)
✅ Cast before string functions: LPAD(CAST(week_number AS STRING), 2, '0')
✅ Handle NULLs in arrays: ARRAY(SELECT x FROM UNNEST([col1, col2, col3]) AS x WHERE x IS NOT NULL)
✅ Aggregate or group all columns: SELECT customer, SUM(revenue) as total_revenue FROM table GROUP BY customer
✅ Use SAFE_CAST for safety: WHERE SAFE_CAST(column AS STRING) = 'value'

DATASET ANALYSIS REQUIREMENTS (BIGQUERY-COMPLIANT):

1. **primary_analysis_sql**: **THE MAIN QUERY TO EXECUTE**
   ```sql
   -- Example structure - MUST be valid BigQuery syntax:
   WITH base_data AS (
     SELECT 
       date_dimension,
       primary_grouping_dimension,
       revenue_column,
       customer_column
     FROM `project.dataset.table`
     WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
   )
   SELECT 
     date_dimension,
     primary_grouping_dimension,
     COUNT(*) as record_count,
     SUM(SAFE_CAST(revenue_column AS NUMERIC)) as total_revenue,
     COUNT(DISTINCT customer_column) as unique_customers,
     AVG(SAFE_CAST(revenue_column AS NUMERIC)) as avg_revenue_per_record
   FROM base_data
   GROUP BY date_dimension, primary_grouping_dimension  -- MUST include all non-aggregated columns
   ORDER BY date_dimension DESC
   LIMIT 100


structure_sql: Understand data structure - HANDLE TYPE MISMATCHES
SELECT 
  'Data Structure Analysis' as analysis_type,
  COUNT(*) as total_records,
  COUNT(DISTINCT SAFE_CAST(date_column AS DATE)) as unique_dates,
  COUNT(DISTINCT customer_column) as unique_customers,
  COUNT(DISTINCT SAFE_CAST(fiscal_week_id AS STRING)) as unique_fiscal_weeks,
  MIN(SAFE_CAST(date_column AS DATE)) as earliest_date,
  MAX(SAFE_CAST(date_column AS DATE)) as latest_date,
  APPROX_COUNT_DISTINCT(primary_key) as approx_unique_records
FROM `project.dataset.main_table` 
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)



validation_sql: Quick validation - USE SAFE_CAST
SELECT 
  'Validation Check' as test_type,
  CASE WHEN SUM(SAFE_CAST(revenue AS NUMERIC)) > 0 THEN 'PASS' ELSE 'FAIL' END as revenue_test,
  CASE WHEN COUNT(DISTINCT customer_column) > 0 THEN 'PASS' ELSE 'FAIL' END as customer_test,
  CASE WHEN MAX(SAFE_CAST(date_column AS DATE)) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) THEN 'PASS' ELSE 'FAIL' END as freshness_test,
  CASE WHEN COUNT(CASE WHEN SAFE_CAST(status_column AS STRING) IN ('CP', 'PY') THEN 1 END) > 0 THEN 'PASS' ELSE 'FAIL' END as status_test
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
LIMIT 1



business_rules_sql: Business logic validation - HANDLE ARRAYS PROPERLY
SELECT 
  'Business Rule Validation' as validation_type,
  'period_type_validation' as rule_name,
  COUNT(*) as records_tested,
  SUM(CASE WHEN SAFE_CAST(period_column AS STRING) IN ('CP', 'PY') THEN 1 ELSE 0 END) as records_passing_rule,
  SAFE_DIVIDE(SUM(CASE WHEN SAFE_CAST(period_column AS STRING) IN ('CP', 'PY') THEN 1 ELSE 0 END), COUNT(*)) * 100 as pass_rate_percentage
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)



sample_data_sql: Representative sample - CAST DATE COLUMNS
SELECT 
  -- Cast all potentially problematic columns
  SAFE_CAST(date_column AS DATE) as date_column,
  customer_dimension,
  product_dimension,
  SAFE_CAST(fiscal_week_id AS STRING) as fiscal_week_id,
  SAFE_CAST(revenue_metric AS NUMERIC) as revenue_metric,
  SAFE_CAST(volume_metric AS NUMERIC) as volume_metric
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY)
  AND revenue_metric IS NOT NULL
ORDER BY SAFE_CAST(date_column AS DATE) DESC, SAFE_CAST(revenue_metric AS NUMERIC) DESC
LIMIT 500


BIGQUERY SYNTAX ENFORCEMENT:

Type Casting Rules:

ALWAYS use SAFE_CAST() instead of implicit conversion
Cast before any comparison: SAFE_CAST(column AS STRING) = 'value'
Cast before string functions: LPAD(CAST(number AS STRING), 2, '0')



Aggregation Rules:

Every non-aggregated column in SELECT must be in GROUP BY
Use SUM, COUNT, AVG, MAX, MIN for calculated fields
When unsure, aggregate the column: SUM(column) or MAX(column)



Array Handling:

Never include NULL in arrays: ARRAY(SELECT x FROM UNNEST([col1, col2]) AS x WHERE x IS NOT NULL)
Use ARRAY_AGG() for creating arrays from query results



Date/Time Functions:

Use CURRENT_DATE() not CURRENT_DATE
Use DATE_SUB(CURRENT_DATE(), INTERVAL n DAY)
Cast date columns: SAFE_CAST(date_col AS DATE)



String Comparisons:

Always cast numbers to strings before string comparison
Use SAFE_CAST to avoid errors: SAFE_CAST(fiscal_week AS STRING) = 'CP'


IMPORTANT NOTES:
primary_analysis_sql is the main SQL to execute - this gives you dashboard metrics with real data
All SQL must be valid BigQuery syntax - test every comparison and function
Use SAFE_CAST extensively - prevents type mismatch errors
Always GROUP BY non-aggregated columns - prevents grouping errors
Handle NULLs in arrays explicitly - prevents array errors
Focus on business logic while ensuring technical correctness
Prioritise metrics marked as is_kpi=true and business_criticality=high
All SQL queries should use appropriate sampling and be cost-optimised
"""



# SQL Utilities
def read_and_replace_sql(file_path, replacements=None):
    """Read SQL file and replace specified text patterns."""
    if replacements is None:
        replacements = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            sql = f.read()
        for old_text, new_text in replacements.items():
            sql = sql.replace(old_text, new_text)
        return sql
    except FileNotFoundError:
        raise FileNotFoundError(f"SQL file not found: {file_path}")
    except Exception as e:
        raise Exception(f"Error reading SQL file {file_path}: {str(e)}")

def execute_bq_query(sql, client=None, project=None, to_dataframe=True):
    """Execute SQL query in BigQuery."""
    if client is None:
        client = bigquery.Client(project=project)
    try:
        if to_dataframe:
            df = client.query(sql).to_dataframe()
            print(f"✓ Query executed successfully - returned {len(df)} rows")
            return df
        else:
            job = client.query(sql)
            job.result()
            print(f"✓ Query executed successfully")
            return job
    except Exception as e:
        print(f"✗ Query execution failed: {str(e)}")
        return None

def run_sql_file(file_path, replacements=None, client=None, project=None):
    """Read SQL file, apply replacements, and execute in BigQuery."""
    if replacements is None:
        replacements = {}
    try:
        sql = read_and_replace_sql(file_path, replacements)
        return execute_bq_query(sql, client, project)
    except Exception as e:
        print(f"✗ Failed to run SQL file {file_path}: {str(e)}")
        return None

def format_emails_for_sql(email_list):
    """Format email list for use in SQL IN clauses."""
    if not email_list:
        return "''"
    formatted_emails = "', '".join(email_list)
    return f"'{formatted_emails}'"

# Vertex AI Batch Processing Utilities
def prepare_batch_input_for_gemini(requests, output_gcs_path, temperature=0.20, max_output_tokens=65000):
    """Prepare batch input in correct format for Gemini models."""
    try:
        batch_requests = []
        for i, request in enumerate(requests):
            batch_requests.append({
                "request": {
                    "contents": [
                        {
                            "role": "user",
                            "parts": [{"text": request["content"]}]
                        }
                    ],
                    "generation_config": {
                        "temperature": temperature,
                        "max_output_tokens": max_output_tokens
                    }
                }
            })
        client = storage.Client()
        bucket_name = output_gcs_path.replace('gs://', '').split('/')[0]
        blob_path = '/'.join(output_gcs_path.replace('gs://', '').split('/')[1:])
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        jsonl_content = ""
        for request in batch_requests:
            jsonl_content += json.dumps(request) + "\n"
        blob.upload_from_string(jsonl_content, content_type='application/json')
        print(f"✓ Batch input uploaded to {output_gcs_path}")
        print(f"  Records: {len(batch_requests)}")
        print(f"  Temperature: {temperature}, Max tokens: {max_output_tokens}")
        return True
    except Exception as e:
        print(f"✗ Failed to prepare batch input: {str(e)}")
        return False

def create_vertex_batch_prediction_job(project, display_name, model_name,
                                      input_gcs_uri, output_gcs_uri,
                                      location="us-central1",
                                      instances_format="jsonl",
                                      predictions_format="jsonl"):
    """Create batch prediction job using Vertex AI v1beta1 API."""
    try:
        api_endpoint = f"{location}-aiplatform.googleapis.com"
        client_options = {"api_endpoint": api_endpoint}
        client = aiplatform_v1beta1.JobServiceClient(client_options=client_options)
        batch_prediction_job = {
            "display_name": display_name,
            "model": model_name,
            "input_config": {
                "instances_format": instances_format,
                "gcs_source": {"uris": [input_gcs_uri]},
            },
            "output_config": {
                "predictions_format": predictions_format,
                "gcs_destination": {"output_uri_prefix": output_gcs_uri},
            }
        }
        parent = f"projects/{project}/locations/{location}"
        response = client.create_batch_prediction_job(
            parent=parent, batch_prediction_job=batch_prediction_job
        )
        print(f"✓ Batch prediction job created: {display_name}")
        print(f"  Job name: {response.name}")
        print(f"  Model: {model_name}")
        print(f"  Input: {input_gcs_uri}")
        print(f"  Output: {output_gcs_uri}")
        return response
    except Exception as e:
        print(f"✗ Batch prediction job creation failed: {str(e)}")
        return None

def monitor_batch_prediction_job_quiet(job_name, location="us-central1"):
    """Monitor batch prediction job status - minimal output"""
    try:
        api_endpoint = f"{location}-aiplatform.googleapis.com"
        client_options = {"api_endpoint": api_endpoint}
        client = aiplatform_v1beta1.JobServiceClient(client_options=client_options)
        
        job = client.get_batch_prediction_job(name=job_name)
        
        if hasattr(job.state, 'name'):
            state_name = job.state.name
        else:
            state_name = str(job.state)
        
        return {
            'name': job.name,
            'display_name': job.display_name,
            'state': state_name,
            'create_time': job.create_time,
            'update_time': job.update_time,
            'error': job.error if hasattr(job, 'error') else None
        }
        
    except Exception as e:
        print(f"✗ Failed to get job status: {str(e)}")
        return None

def wait_for_batch_completion_slick(job_name, location="us-central1", check_interval=10, max_wait=7200):
    """Wait for batch prediction job with slick progress display"""
    print(f"Monitoring batch job: {job_name.split('/')[-1]}")
    start_time = time.time()
    
    completion_states = ['JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED']
    success_states = ['JOB_STATE_SUCCEEDED']
    
    while time.time() - start_time < max_wait:
        status = monitor_batch_prediction_job_quiet(job_name, location)
        
        if status:
            state = status['state']
            elapsed = int(time.time() - start_time)
            elapsed_str = f"{elapsed//60}m {elapsed%60}s"
            
            # Create status line
            if state == 'JOB_STATE_PENDING':
                status_line = f"⏳ PENDING | Elapsed: {elapsed_str}"
            elif state == 'JOB_STATE_RUNNING':
                status_line = f"🏃 RUNNING | Elapsed: {elapsed_str}"
            else:
                status_line = f"📊 {state} | Elapsed: {elapsed_str}"
            
            # Overwrite previous line
            sys.stdout.write(f"\r{status_line}")
            sys.stdout.flush()
            
            # Check if completed
            if state in completion_states:
                sys.stdout.write("\n")  # New line after completion
                if state in success_states:
                    print(f"✓ Job completed successfully!")
                else:
                    print(f"✗ Job completed with status: {state}")
                return status
        
        time.sleep(check_interval)
    
    sys.stdout.write("\n")
    print(f"✗ Job did not complete within {max_wait} seconds")
    return None

def read_batch_prediction_results_fixed(output_gcs_prefix):
    """Read batch prediction results from GCS - fixed version"""
    try:
        client = storage.Client()
        bucket_name = output_gcs_prefix.replace('gs://', '').split('/')[0]
        prefix = '/'.join(output_gcs_prefix.replace('gs://', '').split('/')[1:]).rstrip('/')
        
        print(f"Searching for results in gs://{bucket_name}/{prefix}")
        
        bucket = client.bucket(bucket_name)
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        # Find the main predictions.jsonl files (not incremental ones)
        prediction_files = [blob for blob in blobs 
                          if blob.name.endswith('predictions.jsonl') 
                          and 'incremental' not in blob.name 
                          and blob.size > 1000]
        
        if not prediction_files:
            print("No main predictions.jsonl files found!")
            return []
        
        # Sort by creation time and get the most recent
        prediction_files.sort(key=lambda x: x.time_created, reverse=True)
        
        results = []
        blob = prediction_files[0]
        print(f"Reading: {blob.name}")
        content = blob.download_as_text()
        
        for line_num, line in enumerate(content.strip().split('\n')):
            if line.strip():
                try:
                    result = json.loads(line)
                    results.append(result)
                except json.JSONDecodeError as e:
                    print(f"Error parsing line {line_num}: {e}")
        
        print(f"✓ Successfully read {len(results)} predictions")
        return results
        
    except Exception as e:
        print(f"✗ Failed to read batch results: {e}")
        return []

def run_gemini_batch_fast_slick(
    requests,
    project,
    display_name,
    input_gcs_uri,
    output_gcs_uri,
    model_name="gemini-1.5-flash",
    location="us-central1",
    max_output_tokens=65000,
    temperature=0.20
):
    """Run complete Gemini batch prediction workflow - slick version"""
    
    # Step 1: Prepare and upload input
    print("📤 Preparing batch input...")
    success = prepare_batch_input_for_gemini(requests, input_gcs_uri, temperature, max_output_tokens)
    if not success:
        return None
    
    # Step 2: Create batch job
    print("🚀 Creating batch prediction job...")
    job = create_vertex_batch_prediction_job(
        project=project,
        display_name=display_name,
        model_name=f"publishers/google/models/{model_name}",
        input_gcs_uri=input_gcs_uri,
        output_gcs_uri=output_gcs_uri,
        location=location
    )
    
    if not job:
        return None
    
    # Step 3: Wait for completion with slick progress
    final_status = wait_for_batch_completion_slick(job.name, location)
    
    if final_status and final_status['state'] == 'JOB_STATE_SUCCEEDED':
        print("📊 Reading results...")
        results = read_batch_prediction_results_fixed(output_gcs_uri)
        return results
    else:
        print("❌ Batch job failed or timed out")
        return None


def convert_batch_results_to_dataset(results):
    """Convert batch prediction results to structured dataset - Updated for Standardised Structure"""
    try:
        # Extract responses from batch results
        responses = []
        for i, result in enumerate(results):
            try:
                # Navigate to the actual Gemini response text
                if 'response' in result and 'candidates' in result['response']:
                    candidates = result['response']['candidates']
                    if candidates and len(candidates) > 0:
                        candidate = candidates[0]
                        if 'content' in candidate and 'parts' in candidate['content']:
                            parts = candidate['content']['parts']
                            if parts and len(parts) > 0:
                                response_text = parts[0]['text']
                                responses.append({
                                    'raw_response': response_text,
                                    'status': 'success',
                                    'response_id': i
                                })
                            else:
                                print(f"No parts found in response {i}")
                        else:
                            print(f"No content/parts in response {i}")
                    else:
                        print(f"No candidates in response {i}")
                else:
                    print(f"No response/candidates in result {i}")
                    
            except Exception as e:
                print(f"Error processing result {i}: {e}")
                responses.append({
                    'raw_response': str(result),
                    'status': 'error',
                    'response_id': i
                })
        
        print(f"Extracted {len(responses)} responses")
        
        # Parse JSON responses and flatten into separate datasets
        dashboard_data = []
        metrics_data = []
        metric_interactions_data = []
        dataset_analysis_data = []
        hardcoded_issues_data = []
        
        for response in responses:
            try:
                response_text = response['raw_response']
                
                # Extract JSON from markdown code blocks
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif '{' in response_text and '}' in response_text:
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    json_text = response_text[json_start:json_end]
                else:
                    print(f"No JSON found in response {response['response_id']}")
                    continue
                
                # Parse JSON
                parsed_response = json.loads(json_text)
                
                # Extract dashboard summary
                if 'dashboard_summary' in parsed_response:
                    dashboard_summary = parsed_response['dashboard_summary'].copy()
                    dashboard_summary['response_id'] = response['response_id']
                    
                    # Process primary_data_sources (split semicolon-separated values)
                    if 'primary_data_sources' in dashboard_summary:
                        if isinstance(dashboard_summary['primary_data_sources'], str):
                            dashboard_summary['data_sources_list'] = dashboard_summary['primary_data_sources'].split(';')
                            dashboard_summary['data_sources_count'] = len(dashboard_summary['data_sources_list'])
                        else:
                            dashboard_summary['data_sources_list'] = dashboard_summary['primary_data_sources']
                            dashboard_summary['data_sources_count'] = len(dashboard_summary['primary_data_sources']) if dashboard_summary['primary_data_sources'] else 0
                    
                    # Process hardcoded dates and values (convert to counts for analysis)
                    if 'hardcoded_dates_found' in dashboard_summary:
                        dashboard_summary['hardcoded_dates_count'] = len(dashboard_summary['hardcoded_dates_found']) if dashboard_summary['hardcoded_dates_found'] else 0
                    if 'hardcoded_values_found' in dashboard_summary:
                        dashboard_summary['hardcoded_values_count'] = len(dashboard_summary['hardcoded_values_found']) if dashboard_summary['hardcoded_values_found'] else 0
                    if 'governance_opportunities' in dashboard_summary:
                        dashboard_summary['governance_opportunities_count'] = len(dashboard_summary['governance_opportunities']) if dashboard_summary['governance_opportunities'] else 0
                    
                    dashboard_data.append(dashboard_summary)
                
                # Extract dataset analysis
                if 'dataset_analysis' in parsed_response:
                    dataset_analysis = parsed_response['dataset_analysis'].copy()
                    dataset_analysis['response_id'] = response['response_id']
                    dataset_analysis['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                    
                    # Extract hardcoded issues for separate analysis
                    if 'hardcoded_issues' in dataset_analysis:
                        hardcoded_issues = dataset_analysis['hardcoded_issues']
                        
                        # Process hardcoded dates
                        if 'hardcoded_dates' in hardcoded_issues:
                            for date_issue in hardcoded_issues['hardcoded_dates']:
                                date_issue_row = date_issue.copy()
                                date_issue_row['response_id'] = response['response_id']
                                date_issue_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                                date_issue_row['issue_type'] = 'hardcoded_date'
                                hardcoded_issues_data.append(date_issue_row)
                        
                        # Process hardcoded variables
                        if 'hardcoded_variables' in hardcoded_issues:
                            for var_issue in hardcoded_issues['hardcoded_variables']:
                                var_issue_row = var_issue.copy()
                                var_issue_row['response_id'] = response['response_id']
                                var_issue_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                                var_issue_row['issue_type'] = 'hardcoded_variable'
                                # Process hardcoded_values list
                                if 'hardcoded_values' in var_issue_row:
                                    var_issue_row['hardcoded_values_count'] = len(var_issue_row['hardcoded_values']) if var_issue_row['hardcoded_values'] else 0
                                    var_issue_row['hardcoded_values_text'] = ', '.join(var_issue_row['hardcoded_values']) if var_issue_row['hardcoded_values'] else ''
                                hardcoded_issues_data.append(var_issue_row)
                    
                    dataset_analysis_data.append(dataset_analysis)
                
                # Extract metrics
                if 'metrics' in parsed_response:
                    for metric in parsed_response['metrics']:
                        metric_row = metric.copy()
                        metric_row['response_id'] = response['response_id']
                        metric_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                        metric_row['dashboard_name'] = parsed_response.get('dashboard_summary', {}).get('dashboard_name', '')
                        
                        # Process list fields for easier analysis
                        if 'depends_on_metrics' in metric_row:
                            metric_row['depends_on_metrics_count'] = len(metric_row['depends_on_metrics']) if metric_row['depends_on_metrics'] else 0
                            metric_row['depends_on_metrics_text'] = ', '.join(metric_row['depends_on_metrics']) if metric_row['depends_on_metrics'] else ''
                        
                        if 'data_sources' in metric_row:
                            metric_row['data_sources_count'] = len(metric_row['data_sources']) if metric_row['data_sources'] else 0
                            metric_row['data_sources_text'] = ', '.join(metric_row['data_sources']) if metric_row['data_sources'] else ''
                        
                        if 'filters_applied' in metric_row:
                            metric_row['filters_applied_count'] = len(metric_row['filters_applied']) if metric_row['filters_applied'] else 0
                            metric_row['filters_applied_text'] = ', '.join(metric_row['filters_applied']) if metric_row['filters_applied'] else ''
                        
                        if 'hardcoded_dates_in_metric' in metric_row:
                            metric_row['hardcoded_dates_in_metric_count'] = len(metric_row['hardcoded_dates_in_metric']) if metric_row['hardcoded_dates_in_metric'] else 0
                        
                        if 'hardcoded_values_in_metric' in metric_row:
                            metric_row['hardcoded_values_in_metric_count'] = len(metric_row['hardcoded_values_in_metric']) if metric_row['hardcoded_values_in_metric'] else 0
                        
                        if 'governance_issues' in metric_row:
                            metric_row['governance_issues_count'] = len(metric_row['governance_issues']) if metric_row['governance_issues'] else 0
                            metric_row['governance_issues_text'] = ', '.join(metric_row['governance_issues']) if metric_row['governance_issues'] else ''
                        
                        if 'data_quality_concerns' in metric_row:
                            metric_row['data_quality_concerns_count'] = len(metric_row['data_quality_concerns']) if metric_row['data_quality_concerns'] else 0
                            metric_row['data_quality_concerns_text'] = ', '.join(metric_row['data_quality_concerns']) if metric_row['data_quality_concerns'] else ''
                        
                        metrics_data.append(metric_row)
                
                # Extract metric interactions
                if 'metric_interactions' in parsed_response:
                    for interaction in parsed_response['metric_interactions']:
                        interaction_row = interaction.copy()
                        interaction_row['response_id'] = response['response_id']
                        interaction_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                        
                        # Process related_metrics list
                        if 'related_metrics' in interaction_row:
                            interaction_row['related_metrics_count'] = len(interaction_row['related_metrics']) if interaction_row['related_metrics'] else 0
                            interaction_row['related_metrics_text'] = ', '.join(interaction_row['related_metrics']) if interaction_row['related_metrics'] else ''
                        
                        metric_interactions_data.append(interaction_row)
                
            except json.JSONDecodeError as e:
                print(f"JSON parsing error for response {response['response_id']}: {e}")
                print(f"Response preview: {response['raw_response'][:200]}...")
            except Exception as e:
                print(f"Error processing response {response['response_id']}: {e}")
        
        # Create DataFrames
        dashboards_df = pd.DataFrame(dashboard_data) if dashboard_data else pd.DataFrame()
        metrics_df = pd.DataFrame(metrics_data) if metrics_data else pd.DataFrame()
        metric_interactions_df = pd.DataFrame(metric_interactions_data) if metric_interactions_data else pd.DataFrame()
        dataset_analysis_df = pd.DataFrame(dataset_analysis_data) if dataset_analysis_data else pd.DataFrame()
        hardcoded_issues_df = pd.DataFrame(hardcoded_issues_data) if hardcoded_issues_data else pd.DataFrame()
        
        print(f"✓ Created datasets:")
        print(f"  - Dashboards: {len(dashboards_df)} rows")
        print(f"  - Metrics: {len(metrics_df)} rows")
        print(f"  - Metric Interactions: {len(metric_interactions_df)} rows")
        print(f"  - Dataset Analysis: {len(dataset_analysis_df)} rows")
        print(f"  - Hardcoded Issues: {len(hardcoded_issues_df)} rows")
        
        return {
            'dashboards': dashboards_df,
            'metrics': metrics_df,
            'metric_interactions': metric_interactions_df,
            'dataset_analysis': dataset_analysis_df,
            'hardcoded_issues': hardcoded_issues_df,
            'raw_responses': pd.DataFrame(responses)
        }
        
    except Exception as e:
        print(f"✗ Failed to convert results: {e}")
        return None


# Legacy function for compatibility
def run_gemini_batch_fast(requests, project, display_name, input_gcs_uri,
                         output_gcs_uri, model_name="gemini-2.5-flash",
                         location="us-central1", max_output_tokens=65000,
                         temperature=0.20):
    """Legacy function - redirects to slick version"""
    return run_gemini_batch_fast_slick(requests, project, display_name, input_gcs_uri,
                                      output_gcs_uri, model_name, location, max_output_tokens, temperature)

def prepare_looker_analysis_batch(df):
    """Convert dataframe to batch input format with structured Looker analysis prompt."""
    batch_data = []
    
    for dashboard_id, group in df.groupby('looker_studio_report_id'):
        dashboard_data = {
            "dashboard_id": dashboard_id,
            "dashboard_name": group.iloc[0]['looker_studio_report_name'],
            "dashboard_owner": group.iloc[0]['assetOwner'],
            "sql_samples": []
        }
        
        for _, row in group.iterrows():
            dashboard_data["sql_samples"].append({
                "job_id": row['jobId'],
                "username": row['username'],
                "runtime_seconds": row['runtime_seconds'],
                "total_processed_bytes": row['totalProcessedBytes'] if pd.notna(row['totalProcessedBytes']) else None,  # Handle NaN
                "sql_query": row['query_text']
            })
        
        # Format the structured prompt for this dashboard
        formatted_prompt = LOOKER_ANALYSIS_PROMPT.format(
            dashboard_id=dashboard_data["dashboard_id"],
            dashboard_name=dashboard_data["dashboard_name"],
            sql_samples=json.dumps(dashboard_data["sql_samples"], indent=2, default=str)  # Add default=str
        )
        
        batch_data.append({"content": formatted_prompt})
    
    return batch_data


def save_datasets_to_csv(datasets, output_folder="./data/"):
    """Save datasets to CSV files"""
    try:
        import os
        os.makedirs(output_folder, exist_ok=True)

        # Save each dataset
        for name, df in datasets.items():
            if df is not None and len(df) > 0:
                output_path = f"{output_folder}looker_analysis_{name}.csv"
                df.to_csv(output_path, index=False)
                print(f"✓ Saved {name}: {output_path} ({len(df)} rows)")
            else:
                print(f"⚠️ Skipped {name}: empty dataset")
        
        return True 
        
    except Exception as e:
        print(f"✗ Failed to save datasets: {e}")
        return False

def analyse_results_summary(datasets):
    """Quick analysis of the results"""
    print("\n" + "="*60)
    print("LOOKER ANALYSIS RESULTS SUMMARY")
    print("="*60)
    
    if 'dashboards' in datasets and len(datasets['dashboards']) > 0:
        dashboards_df = datasets['dashboards']
        print(f"\n📊 DASHBOARDS ANALYSED: {len(dashboards_df)}")
        
        if 'business_domain' in dashboards_df.columns:
            print("\nDomains:")
            print(dashboards_df['business_domain'].value_counts())
        
        if 'complexity_score' in dashboards_df.columns:
            print(f"\nComplexity scores:")
            print(f"  Average: {dashboards_df['complexity_score'].mean():.1f}")
            print(f"  Range: {dashboards_df['complexity_score'].min()} - {dashboards_df['complexity_score'].max()}")
    
    if 'metrics' in datasets and len(datasets['metrics']) > 0:
        metrics_df = datasets['metrics']
        print(f"\n📈 METRICS IDENTIFIED: {len(metrics_df)}")
        
        if 'metric_type' in metrics_df.columns:
            print("\nMetric types:")
            print(metrics_df['metric_type'].value_counts())
        
        if 'calculation_type' in metrics_df.columns:
            print("\nCalculation types:")
            print(metrics_df['calculation_type'].value_counts())
        
        if 'is_final_output' in metrics_df.columns:
            final_outputs = len(metrics_df[metrics_df['is_final_output'] == True])
            print(f"\nFinal output metrics: {final_outputs}")
    
    if 'raw_responses' in datasets and len(datasets['raw_responses']) > 0:
        raw_df = datasets['raw_responses']
        successful = len(raw_df[raw_df['status'] == 'success'])
        failed = len(raw_df[raw_df['status'] == 'error'])
        print(f"\n📈 PROCESSING SUMMARY:")
        print(f"  Successful responses: {successful}")
        print(f"  Failed responses: {failed}")
    
    print("\n" + "="*60)


def execute_sql_with_metadata(sql, query_type, dashboard_id, response_id, client=None, project=None):
    """Execute a single SQL query and return results with metadata"""
    import time
    from datetime import datetime
    
    if client is None:
        from google.cloud import bigquery
        client = bigquery.Client(project=project)
    
    start_time = time.time()
    
    try:
        # Execute the query
        job = client.query(sql)
        results = job.result()
        
        # Convert to DataFrame
        df = results.to_dataframe()
        
        execution_time = time.time() - start_time
        
        # Create metadata
        metadata = {
            'dashboard_id': dashboard_id,
            'response_id': response_id,
            'query_type': query_type,
            'execution_status': 'success',
            'execution_time_seconds': round(execution_time, 2),
            'row_count': len(df),
            'column_count': len(df.columns),
            'columns': list(df.columns),
            'executed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'bytes_processed': job.total_bytes_processed if hasattr(job, 'total_bytes_processed') else None,
            'slot_ms': job.slot_millis if hasattr(job, 'slot_millis') else None,
            'error_message': None
        }
        
        print(f"✓ {query_type} for {dashboard_id}: {len(df)} rows, {execution_time:.2f}s")
        
        return {
            'data': df,
            'metadata': metadata,
            'sql': sql
        }
        
    except Exception as e:
        execution_time = time.time() - start_time
        
        metadata = {
            'dashboard_id': dashboard_id,
            'response_id': response_id,
            'query_type': query_type,
            'execution_status': 'failed',
            'execution_time_seconds': round(execution_time, 2),
            'row_count': 0,
            'column_count': 0,
            'columns': [],
            'executed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'bytes_processed': None,
            'slot_ms': None,
            'error_message': str(e)
        }
        
        print(f"✗ {query_type} for {dashboard_id}: FAILED - {str(e)[:100]}...")
        
        return {
            'data': None,
            'metadata': metadata,
            'sql': sql
        }

def execute_dataset_analysis_queries(dataset_df, client=None, project=None, max_queries=None):
    """Execute all SQL queries from the dataset analysis DataFrame"""
    import pandas as pd
    
    if client is None:
        from google.cloud import bigquery
        client = bigquery.Client(project=project)
    
    # Query columns to process
    query_columns = ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']
    
    all_results = []
    all_metadata = []
    
    # Limit queries if specified
    rows_to_process = dataset_df.head(max_queries) if max_queries else dataset_df
    
    print(f"Executing queries for {len(rows_to_process)} dashboards...")
    
    for idx, row in rows_to_process.iterrows():
        dashboard_id = row['dashboard_id']
        response_id = row['response_id']
        
        print(f"\n--- Processing Dashboard {idx + 1}/{len(rows_to_process)}: {dashboard_id[:8]}... ---")
        
        for query_type in query_columns:
            sql = row[query_type]
            
            if pd.isna(sql) or sql.strip() == '':
                print(f"⚠️ Skipping {query_type}: empty SQL")
                continue
            
            # Execute the query
            result = execute_sql_with_metadata(
                sql=sql,
                query_type=query_type,
                dashboard_id=dashboard_id,
                response_id=response_id,
                client=client,
                project=project
            )
            
            # Store results
            if result['data'] is not None:
                # Add linking columns to the data
                result['data']['dashboard_id'] = dashboard_id
                result['data']['response_id'] = response_id
                result['data']['query_type'] = query_type
                
                all_results.append({
                    'dashboard_id': dashboard_id,
                    'response_id': response_id,
                    'query_type': query_type,
                    'data': result['data']
                })
            
            all_metadata.append(result['metadata'])
    
    # Create metadata DataFrame
    metadata_df = pd.DataFrame(all_metadata)
    
    print(f"\n=== EXECUTION SUMMARY ===")
    print(f"Total queries attempted: {len(all_metadata)}")
    print(f"Successful queries: {len([m for m in all_metadata if m['execution_status'] == 'success'])}")
    print(f"Failed queries: {len([m for m in all_metadata if m['execution_status'] == 'failed'])}")
    
    if len(all_metadata) > 0:
        avg_time = sum([m['execution_time_seconds'] for m in all_metadata]) / len(all_metadata)
        total_rows = sum([m['row_count'] for m in all_metadata])
        print(f"Average execution time: {avg_time:.2f}s")
        print(f"Total rows returned: {total_rows}")
    
    return {
        'results': all_results,
        'metadata': metadata_df,
        'summary': {
            'total_queries': len(all_metadata),
            'successful_queries': len([m for m in all_metadata if m['execution_status'] == 'success']),
            'failed_queries': len([m for m in all_metadata if m['execution_status'] == 'failed']),
            'total_rows_returned': sum([m['row_count'] for m in all_metadata])
        }
    }

def combine_query_results_by_type(execution_results):
    """Combine all results by query type for easier analysis"""
    import pandas as pd
    
    results_by_type = {}
    
    for result in execution_results['results']:
        query_type = result['query_type']
        
        if query_type not in results_by_type:
            results_by_type[query_type] = []
        
        results_by_type[query_type].append(result['data'])
    
    # Combine DataFrames for each query type
    combined_results = {}
    for query_type, dfs in results_by_type.items():
        if dfs:
            try:
                combined_df = pd.concat(dfs, ignore_index=True)
                combined_results[query_type] = combined_df
                print(f"✓ Combined {query_type}: {len(combined_df)} total rows from {len(dfs)} dashboards")
            except Exception as e:
                print(f"✗ Failed to combine {query_type}: {e}")
                combined_results[query_type] = None
    
    return combined_results

def analyze_query_performance(metadata_df):
    """Analyze the performance of executed queries"""
    import pandas as pd
    
    if metadata_df.empty:
        print("No metadata to analyze")
        return None
    
    print("\n=== QUERY PERFORMANCE ANALYSIS ===")
    
    # Success rates by query type
    success_by_type = metadata_df.groupby('query_type')['execution_status'].apply(
        lambda x: (x == 'success').sum() / len(x) * 100
    ).round(2)
    
    print("\nSuccess rates by query type:")
    for query_type, success_rate in success_by_type.items():
        print(f"  {query_type}: {success_rate}%")
    
    # Execution times by query type (successful only)
    successful_queries = metadata_df[metadata_df['execution_status'] == 'success']
    
    if not successful_queries.empty:
        print("\nExecution times (successful queries only):")
        time_stats = successful_queries.groupby('query_type')['execution_time_seconds'].agg(['mean', 'min', 'max']).round(2)
        for query_type, stats in time_stats.iterrows():
            print(f"  {query_type}: avg={stats['mean']}s, min={stats['min']}s, max={stats['max']}s")
        
        print("\nRow counts by query type:")
        row_stats = successful_queries.groupby('query_type')['row_count'].agg(['mean', 'min', 'max']).round(0)
        for query_type, stats in row_stats.iterrows():
            print(f"  {query_type}: avg={stats['mean']} rows, min={stats['min']}, max={stats['max']}")
    
    # Failed queries details
    failed_queries = metadata_df[metadata_df['execution_status'] == 'failed']
    if not failed_queries.empty:
        print(f"\n=== FAILED QUERIES ({len(failed_queries)}) ===")
        for _, row in failed_queries.iterrows():
            print(f"  {row['query_type']} [{row['dashboard_id'][:8]}...]: {row['error_message'][:100]}...")
    
    return {
        'success_rates': success_by_type,
        'performance_stats': time_stats if not successful_queries.empty else None,
        'failed_queries': failed_queries
    }

# Example usage function
def run_complete_analysis(dataset_df, project=None, max_dashboards=5):
    """Run complete analysis with all queries"""
    from google.cloud import bigquery
    
    # Create BigQuery client
    client = bigquery.Client(project=project) if project else bigquery.Client()
    
    print(f"Starting complete analysis for {min(max_dashboards or len(dataset_df), len(dataset_df))} dashboards...")
    
    # Execute all queries
    execution_results = execute_dataset_analysis_queries(
        dataset_df, 
        client=client, 
        project=project, 
        max_queries=max_dashboards
    )
    
    # Analyze performance
    performance_analysis = analyze_query_performance(execution_results['metadata'])
    
    # Combine results by type
    combined_results = combine_query_results_by_type(execution_results)
    
    return {
        'execution_results': execution_results,
        'combined_results': combined_results,
        'performance_analysis': performance_analysis
    }



def troubleshoot_failed_queries(execution_results):
    """Analyze and categorize failed queries to improve the prompt"""
    
    failed_queries = execution_results['metadata'][execution_results['metadata']['execution_status'] == 'failed']
    
    if failed_queries.empty:
        print("No failed queries to troubleshoot!")
        return
    
    print("=== FAILED QUERY ANALYSIS ===\n")
    
    error_categories = {
        'type_mismatch': [],
        'function_signature': [],
        'group_by_issues': [],
        'array_issues': [],
        'other': []
    }
    
    for _, row in failed_queries.iterrows():
        error_msg = row['error_message'].lower()
        
        if 'no matching signature for operator' in error_msg and ('int64' in error_msg or 'string' in error_msg):
            error_categories['type_mismatch'].append(row)
        elif 'no matching signature for function' in error_msg:
            error_categories['function_signature'].append(row)
        elif 'neither grouped nor' in error_msg or 'group by' in error_msg:
            error_categories['group_by_issues'].append(row)
        elif 'array cannot have a null element' in error_msg:
            error_categories['array_issues'].append(row)
        else:
            error_categories['other'].append(row)
    
    # Print categorized errors
    for category, errors in error_categories.items():
        if errors:
            print(f"\n{category.upper().replace('_', ' ')} ({len(errors)} errors):")
            for error in errors:
                print(f"  Dashboard: {error['dashboard_id'][:8]}...")
                print(f"  Query Type: {error['query_type']}")
                print(f"  Error: {error['error_message'][:150]}...")
                print()
    
    return error_categories

def get_failed_sql_for_inspection(dataset_df, execution_results, error_type='type_mismatch'):
    """Get the actual SQL from failed queries for inspection"""
    
    failed_metadata = execution_results['metadata'][execution_results['metadata']['execution_status'] == 'failed']
    
    print(f"=== FAILED SQL INSPECTION ({error_type.upper()}) ===\n")
    
    for _, row in failed_metadata.head(2).iterrows():  # Show first 2 failed queries
        dashboard_id = row['dashboard_id']
        query_type = row['query_type']
        
        # Get the SQL from original dataset
        original_row = dataset_df[dataset_df['dashboard_id'] == dashboard_id].iloc[0]
        sql = original_row[query_type]
        
        print(f"Dashboard: {dashboard_id}")
        print(f"Query Type: {query_type}")
        print(f"Error: {row['error_message']}")
        print("\nSQL:")
        print("-" * 80)
        print(sql[:500] + "..." if len(sql) > 500 else sql)
        print("-" * 80)
        print("\n")

def auto_git_push(commit_message=None, files_to_add=".", include_timestamp=True, 
                  dry_run=False, force_push=False):
    """
    Automatically commit and push changes to git
    
    Args:
        commit_message (str): Custom commit message
        files_to_add (str): Files to add ('.' for all, or specific files)
        include_timestamp (bool): Add timestamp to commit message
        dry_run (bool): Show what would be done without doing it
        force_push (bool): Force push even if no changes detected
    """
    import subprocess
    import os
    from datetime import datetime
    
    def run_git_command(command, capture_output=True):
        """Run git command and return result"""
        try:
            if dry_run:
                print(f"[DRY RUN] Would run: {command}")
                return True, ""
            
            result = subprocess.run(command, shell=True, capture_output=capture_output, 
                                  text=True, cwd='/content/looker-metrics')
            return result.returncode == 0, result.stdout + result.stderr
        except Exception as e:
            return False, str(e)
    
    print("🔄 Auto Git Push Starting...")
    
    # Check if we're in a git repository
    success, output = run_git_command("git status --porcelain")
    if not success:
        print("❌ Not in a git repository or git error occurred")
        return False
    
    # Check if there are changes
    if not output.strip() and not force_push:
        print("✅ No changes to commit")
        return True
    
    print(f"📁 Changes detected:")
    if not dry_run:
        run_git_command("git status --short", capture_output=False)
    
    # Add files
    print(f"📤 Adding files: {files_to_add}")
    success, output = run_git_command(f"git add {files_to_add}")
    if not success:
        print(f"❌ Failed to add files: {output}")
        return False
    
    # Create commit message
    if commit_message is None:
        commit_message = "Auto-commit: Updated looker analysis code"
    
    if include_timestamp:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        commit_message = f"{commit_message} - {timestamp}"
    
    # Commit changes
    print(f"💾 Committing: {commit_message}")
    success, output = run_git_command(f'git commit -m "{commit_message}"')
    if not success:
        if "nothing to commit" in output:
            print("✅ Nothing new to commit")
            return True
        else:
            print(f"❌ Failed to commit: {output}")
            return False
    
    # Push to remote
    print("🚀 Pushing to GitHub...")
    success, output = run_git_command("git push origin main")
    if not success:
        print(f"❌ Failed to push: {output}")
        return False
    
    print("✅ Successfully pushed to GitHub!")
    return True

def setup_auto_git_config(username, email, token):
    """
    One-time setup for git configuration and authentication
    
    Args:
        username (str): Your GitHub username
        email (str): Your email address  
        token (str): Your GitHub Personal Access Token
    """
    import subprocess
    
    def run_command(command):
        try:
            result = subprocess.run(command, shell=True, capture_output=True, 
                                  text=True, cwd='/content/looker-metrics')
            return result.returncode == 0, result.stdout + result.stderr
        except Exception as e:
            return False, str(e)
    
    print("⚙️ Setting up git configuration...")
    
    # Set user identity
    run_command(f'git config --global user.email "{email}"')
    run_command(f'git config --global user.name "{username}"')
    
    # Set remote URL with authentication
    repo_url = f"https://{username}:{token}@github.com/richie-peters/looker-metrics.git"
    success, output = run_command(f'git remote set-url origin {repo_url}')
    
    if success:
        print("✅ Git configuration complete!")
        return True
    else:
        print(f"❌ Git configuration failed: {output}")
        return False

def create_auto_backup_decorator():
    """
    Create a decorator that automatically backs up after running functions
    """
    def auto_backup_decorator(func):
        def wrapper(*args, **kwargs):
            # Run the original function
            result = func(*args, **kwargs)
            
            # Auto-backup after function completes
            print(f"\n🔄 Auto-backup after {func.__name__}...")
            auto_git_push(
                commit_message=f"Auto-backup after running {func.__name__}",
                include_timestamp=True
            )
            
            return result
        return wrapper
    return auto_backup_decorator

# Usage examples and setup
def setup_git_auto_push():
    """Setup function - run this once"""
    print("Setting up auto-git-push...")
    
    # You'll need to provide these
    USERNAME = "richie-peters"
    EMAIL = "richie.peters@news.com.au"  
    TOKEN = "your_github_token_here"  # Replace with your actual token
    
    # Setup git config
    setup_auto_git_config(USERNAME, EMAIL, TOKEN)
    
    print("\n🎯 Auto-push functions ready to use!")
    print("\nUsage examples:")
    print("  auto_git_push()  # Simple push with default message")
    print("  auto_git_push('Fixed bug in analysis')  # Custom message")
    print("  auto_git_push(dry_run=True)  # See what would happen")

# Convenience functions
def quick_push(message="Quick update"):
    """Quick push with simple message"""
    return auto_git_push(commit_message=message)

def save_progress(description="Progress checkpoint"):
    """Save current progress"""
    return auto_git_push(commit_message=f"Progress: {description}")

def backup_now():
    """Emergency backup"""
    return auto_git_push(commit_message="Emergency backup", force_push=True)

# Example: Auto-backup decorator usage
@create_auto_backup_decorator()
def my_analysis_function():
    """Example function that auto-backs up when it finishes"""
    print("Doing some analysis...")
    # Your analysis code here
    return "Analysis complete"

print("✅ Auto-git-push functions loaded!")
print("\n🚀 To get started:")
print("1. Run: setup_git_auto_push()  # One-time setup")
print("2. Then use: auto_git_push() or quick_push('your message')")

def fix_github_authentication():
    """Fix GitHub authentication with Personal Access Token"""
    
    # You need to paste your actual token here
    USERNAME = "richie-peters"
    TOKEN = input("Paste your GitHub Personal Access Token here: ")  # This will prompt you to enter it
    
    if not TOKEN or TOKEN.strip() == "":
        print("❌ No token provided!")
        return False
    
    # Remove any whitespace
    TOKEN = TOKEN.strip()
    
    # Set the remote URL with token authentication
    import subprocess
    
    repo_url = f"https://{USERNAME}:{TOKEN}@github.com/richie-peters/looker-metrics.git"
    
    try:
        # Update the remote URL
        result = subprocess.run(
            f'git remote set-url origin {repo_url}', 
            shell=True, 
            capture_output=True, 
            text=True, 
            cwd='/content/looker-metrics'
        )
        
        if result.returncode == 0:
            print("✅ GitHub authentication updated successfully!")
            
            # Test the connection
            test_result = subprocess.run(
                'git remote -v', 
                shell=True, 
                capture_output=True, 
                text=True, 
                cwd='/content/looker-metrics'
            )
            
            if "https://" in test_result.stdout:
                print("✅ Remote URL configured correctly")
                return True
            else:
                print("⚠️ Remote URL might not be set correctly")
                return False
        else:
            print(f"❌ Failed to set remote URL: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"❌ Error setting up authentication: {e}")
        return False


def check_git_status():
    """Check detailed git status"""
    import subprocess
    import os
    
    os.chdir('/content/looker-metrics')
    
    print("=== CURRENT GIT STATUS ===")
    
    # Check working directory status
    result = subprocess.run('git status', shell=True, capture_output=True, text=True)
    print("Working directory status:")
    print(result.stdout)
    
    # Check if there are unpushed commits
    print("\n=== UNPUSHED COMMITS ===")
    result = subprocess.run('git log origin/main..HEAD --oneline', shell=True, capture_output=True, text=True)
    if result.stdout.strip():
        print("Unpushed commits:")
        print(result.stdout)
    else:
        print("No unpushed commits")
    
    # Check recent commits
    print("\n=== RECENT COMMITS ===")
    result = subprocess.run('git log --oneline -5', shell=True, capture_output=True, text=True)
    print(result.stdout)


def push_existing_commits():
    """Push any existing unpushed commits"""
    import subprocess
    import os
    
    os.chdir('/content/looker-metrics')
    
    # Check for unpushed commits
    result = subprocess.run('git log origin/main..HEAD --oneline', shell=True, capture_output=True, text=True)
    
    if result.stdout.strip():
        print("📤 Found unpushed commits, pushing now...")
        push_result = subprocess.run('git push origin main', shell=True, capture_output=True, text=True)
        
        if push_result.returncode == 0:
            print("✅ Successfully pushed existing commits!")
            return True
        else:
            print(f"❌ Push failed: {push_result.stderr}")
            return False
    else:
        print("✅ No unpushed commits found")
        return True

def create_unified_dataset_from_existing(datasets, df_bq_results=None):
    """
    Create unified dataset from your existing separate datasets
    """
    import pandas as pd
    
    print("🔗 CREATING UNIFIED DATASET FROM YOUR EXISTING DATA")
    print("=" * 55)
    
    # Get your datasets
    dashboards_df = datasets.get('dashboards', pd.DataFrame())
    metrics_df = datasets.get('metrics', pd.DataFrame()) 
    metric_interactions_df = datasets.get('metric_interactions', pd.DataFrame())
    dataset_analysis_df = datasets.get('dataset_analysis', pd.DataFrame())
    hardcoded_issues_df = datasets.get('hardcoded_issues', pd.DataFrame())
    
    print(f"Input datasets:")
    print(f"  Dashboards: {len(dashboards_df)} rows")
    print(f"  Metrics: {len(metrics_df)} rows") 
    print(f"  Metric Interactions: {len(metric_interactions_df)} rows")
    print(f"  Dataset Analysis: {len(dataset_analysis_df)} rows")
    print(f"  Hardcoded Issues: {len(hardcoded_issues_df)} rows")
    
    unified_records = []
    
    # Create dashboard-level records
    print("\n📊 Creating dashboard-level records...")
    for _, dashboard in dashboards_df.iterrows():
        dashboard_id = dashboard['dashboard_id']
        
        # Get related data for this dashboard
        dashboard_metrics = metrics_df[metrics_df['dashboard_id'] == dashboard_id] if not metrics_df.empty else pd.DataFrame()
        dashboard_interactions = metric_interactions_df[metric_interactions_df['dashboard_id'] == dashboard_id] if not metric_interactions_df.empty else pd.DataFrame()
        dashboard_analysis = dataset_analysis_df[dataset_analysis_df['dashboard_id'] == dashboard_id] if not dataset_analysis_df.empty else pd.DataFrame()
        dashboard_issues = hardcoded_issues_df[hardcoded_issues_df['dashboard_id'] == dashboard_id] if not hardcoded_issues_df.empty else pd.DataFrame()
        
        # Create dashboard summary record  
        dashboard_record = {
            # Core identifiers
            'record_id': f"{dashboard_id}_dashboard",
            'dashboard_id': dashboard_id,
            'response_id': dashboard.get('response_id', ''),
            'record_type': 'dashboard_summary',
            
            # Dashboard info
            'dashboard_name': dashboard.get('dashboard_name', ''),
            'business_domain': dashboard.get('business_domain', ''),
            'complexity_score': dashboard.get('complexity_score', 0),
            'consolidation_score': dashboard.get('consolidation_score', 0),
            'date_grain': dashboard.get('date_grain', ''),
            'data_grain': dashboard.get('data_grain', ''),
            'primary_data_sources': dashboard.get('primary_data_sources', ''),
            'date_range_detected': dashboard.get('date_range_detected', ''),
            
            # Aggregated metrics info
            'total_metrics_count': len(dashboard_metrics),
            'kpi_metrics_count': len(dashboard_metrics[dashboard_metrics.get('is_kpi', False) == True]) if not dashboard_metrics.empty else 0,
            'final_output_metrics_count': len(dashboard_metrics[dashboard_metrics.get('is_final_output', False) == True]) if not dashboard_metrics.empty else 0,
            
            # Governance summary
            'hardcoded_dates_count': dashboard.get('hardcoded_dates_count', 0),
            'hardcoded_values_count': dashboard.get('hardcoded_values_count', 0),
            'governance_issues_count': len(dashboard_issues),
            
            # Interactions
            'metric_interactions_count': len(dashboard_interactions),
            
            # Text summaries for embeddings
            'dashboard_description': create_dashboard_description(dashboard, dashboard_metrics),
            'metrics_summary': create_metrics_summary_text(dashboard_metrics),
            'governance_summary': create_governance_summary_text(dashboard_issues),
            'full_context': 'dashboard_summary'
        }
        
        unified_records.append(dashboard_record)
        
        # Create individual metric records
        for _, metric in dashboard_metrics.iterrows():
            metric_record = {
                'record_id': f"{dashboard_id}_{metric.get('metric_id', 'unknown')}",
                'dashboard_id': dashboard_id,
                'response_id': metric.get('response_id', ''),
                'record_type': 'metric',
                
                # Link to dashboard
                'dashboard_name': dashboard.get('dashboard_name', ''),
                'business_domain': dashboard.get('business_domain', ''),
                
                # Metric details
                'metric_id': metric.get('metric_id', ''),
                'metric_name': metric.get('metric_name', ''),
                'metric_type': metric.get('metric_type', ''),
                'calculation_type': metric.get('calculation_type', ''),
                'is_kpi': metric.get('is_kpi', False),
                'is_final_output': metric.get('is_final_output', False),
                'business_criticality': metric.get('business_criticality', ''),
                'metric_category': metric.get('metric_category', ''),
                'business_description': metric.get('business_description', ''),
                'sql_logic': metric.get('sql_logic', ''),
                
                # Dependencies and relationships
                'depends_on_metrics_count': metric.get('depends_on_metrics_count', 0),
                'data_sources_count': metric.get('data_sources_count', 0),
                'governance_issues_count': metric.get('governance_issues_count', 0),
                
                # Text description
                'dashboard_description': create_metric_description(metric, dashboard),
                'metrics_summary': metric.get('business_description', ''),
                'governance_summary': metric.get('governance_issues_text', ''),
                'full_context': 'individual_metric'
            }
            
            unified_records.append(metric_record)
    
    # Convert to DataFrame
    unified_df = pd.DataFrame(unified_records)
    
    print(f"\n✅ Created unified dataset:")
    print(f"   Total records: {len(unified_df)}")
    print(f"   Dashboard summaries: {len(unified_df[unified_df['record_type'] == 'dashboard_summary'])}")
    print(f"   Individual metrics: {len(unified_df[unified_df['record_type'] == 'metric'])}")
    print(f"   Columns: {len(unified_df.columns)}")
    
    return unified_df

def create_dashboard_description(dashboard, metrics):
    """Create readable description of dashboard"""
    parts = []
    
    name = dashboard.get('dashboard_name', 'Unknown Dashboard')
    domain = dashboard.get('business_domain', 'unknown')
    
    parts.append(f"{name} is a {domain} dashboard")
    
    if not metrics.empty:
        kpis = len(metrics[metrics.get('is_kpi', False) == True])
        total = len(metrics)
        parts.append(f"containing {total} metrics")
        if kpis > 0:
            parts.append(f"including {kpis} key performance indicators")
    
    complexity = dashboard.get('complexity_score', 0)
    if complexity > 7:
        parts.append("with high analytical complexity")
    elif complexity > 4:
        parts.append("with moderate complexity")
    
    sources = dashboard.get('primary_data_sources', '')
    if sources:
        source_count = len(sources.split(';'))
        parts.append(f"drawing from {source_count} data sources")
    
    return '. '.join(parts) + '.'

def create_metrics_summary_text(metrics):
    """Create text summary of metrics"""
    if metrics.empty:
        return "No metrics defined."
    
    parts = []
    total = len(metrics)
    kpis = len(metrics[metrics.get('is_kpi', False) == True])
    
    parts.append(f"Contains {total} metrics")
    if kpis > 0:
        parts.append(f"{kpis} are key performance indicators")
    
    if 'metric_category' in metrics.columns:
        categories = metrics['metric_category'].value_counts()
        if not categories.empty:
            top_cat = categories.index[0]
            parts.append(f"primarily focused on {top_cat} metrics")
    
    return '. '.join(parts) + '.'

def create_governance_summary_text(issues):
    """Create governance issues summary"""
    if issues.empty:
        return "No governance issues identified."
    
    parts = []
    total = len(issues)
    parts.append(f"Has {total} governance issues")
    
    if 'issue_type' in issues.columns:
        issue_types = issues['issue_type'].value_counts()
        for issue_type, count in issue_types.head(2).items():
            parts.append(f"{count} {issue_type.replace('_', ' ')} issues")
    
    return '. '.join(parts) + '.'

def create_metric_description(metric, dashboard):
    """Create description for individual metric"""
    parts = []
    
    name = metric.get('metric_name', 'Unknown Metric')
    mtype = metric.get('metric_type', 'unknown')
    calc_type = metric.get('calculation_type', 'unknown')
    
    parts.append(f"{name} is a {mtype} metric using {calc_type} calculation")
    
    if metric.get('is_kpi'):
        parts.append("classified as a key performance indicator")
    
    if metric.get('business_description'):
        parts.append(f"measuring {metric['business_description']}")
    
    parts.append(f"from the {dashboard.get('dashboard_name', 'unknown')} dashboard")
    
    return '. '.join(parts) + '.'

def prepare_secondary_batch_input_clean(unified_dataset, df_bq_results):
    """
    Clean version - only use dashboard summary records
    """
    import json
    
    print("📊 PREPARING SECONDARY BATCH INPUT (CLEAN VERSION)")
    print("=" * 55)
    
    # Filter to only dashboard summary records and remove NaN columns
    dashboard_summaries = unified_dataset[
        unified_dataset['record_type'] == 'dashboard_summary'
    ].dropna(axis=1, how='all')
    
    print(f"Processing {len(dashboard_summaries)} dashboard summaries")
    
    secondary_batch_data = []
    
    # Get the combined BQ results
    combined_results = df_bq_results.get('combined_results', {})
    
    for _, dashboard in dashboard_summaries.iterrows():
        dashboard_id = dashboard['dashboard_id']
        
        print(f"  Processing: {dashboard.get('dashboard_name', 'Unknown')[:50]}...")
        
        # Get original analysis data - only use non-NaN values
        original_analysis = {}
        for key in ['dashboard_name', 'business_domain', 'complexity_score', 'consolidation_score', 
                   'total_metrics_count', 'primary_data_sources', 'date_grain', 'data_grain',
                   'kpi_metrics_count', 'governance_issues_count']:
            value = dashboard.get(key)
            if pd.notna(value):
                original_analysis[key] = value
        
        original_analysis['dashboard_id'] = dashboard_id
        
        # Get actual SQL results for this dashboard
        actual_results = {}
        
        for query_type in ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']:
            if query_type in combined_results and combined_results[query_type] is not None:
                dashboard_data = combined_results[query_type][
                    combined_results[query_type]['dashboard_id'] == dashboard_id
                ]
                
                if not dashboard_data.empty:
                    # Convert to JSON-serializable format
                    actual_results[query_type] = {
                        'row_count': len(dashboard_data),
                        'columns': [col for col in dashboard_data.columns if col not in ['dashboard_id', 'response_id', 'query_type']],
                        'sample_data': dashboard_data.head(3).to_dict('records'),
                        'data_summary': f"Dataset contains {len(dashboard_data)} rows with {len(dashboard_data.columns)} columns"
                    }
                else:
                    actual_results[query_type] = {'row_count': 0, 'message': 'No data returned'}
            else:
                actual_results[query_type] = {'message': 'Query not executed or failed'}
        
        # Get related metrics for this dashboard from the unified dataset
        dashboard_metrics = unified_dataset[
            (unified_dataset['dashboard_id'] == dashboard_id) & 
            (unified_dataset['record_type'] == 'metric')
        ]
        
        metrics_info = []
        for _, metric in dashboard_metrics.iterrows():
            metric_dict = {}
            for key in ['metric_id', 'metric_name', 'metric_type', 'business_description', 
                       'sql_logic', 'is_kpi', 'calculation_type', 'metric_category']:
                value = metric.get(key)
                if pd.notna(value):
                    metric_dict[key] = value
            if metric_dict:  # Only add if we have some data
                metrics_info.append(metric_dict)
        
        # Create the input data structure
        secondary_input_data = {
            'dashboard_id': dashboard_id,
            'dashboard_name': dashboard.get('dashboard_name', ''),
            'original_dashboard_analysis': original_analysis,
            'dashboard_metrics': metrics_info,
            'actual_sql_results': actual_results
        }
        
        # Get the secondary prompt
        secondary_prompt = design_secondary_analysis_prompt()
        
        # Format the prompt with the data
        try:
            formatted_prompt = secondary_prompt.format(
                dashboard_id=dashboard_id,
                dashboard_name=dashboard.get('dashboard_name', ''),
                original_dashboard_analysis=json.dumps(original_analysis, indent=2),
                actual_sql_results=json.dumps(actual_results, indent=2),
                primary_analysis_data=json.dumps(actual_results.get('primary_analysis_sql', {}), indent=2),
                structure_analysis_data=json.dumps(actual_results.get('structure_sql', {}), indent=2),
                validation_results=json.dumps(actual_results.get('validation_sql', {}), indent=2),
                business_rules_data=json.dumps(actual_results.get('business_rules_sql', {}), indent=2)
            )
            
            secondary_batch_data.append({
                'content': formatted_prompt,
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard.get('dashboard_name', '')
            })
            
        except Exception as e:
            print(f"    ⚠️ Error formatting prompt for {dashboard_id}: {e}")
            continue
    
    print(f"✅ Successfully prepared {len(secondary_batch_data)} secondary analysis requests")
    return secondary_batch_data


def design_secondary_analysis_prompt():
    """
    Design the secondary analysis prompt that leverages actual SQL results
    for consolidation planning and migration preparation
    """
    
    secondary_prompt = """
    SECONDARY LOOKER CONSOLIDATION ANALYSIS
    =====================================
    
    You are an expert data architect analyzing Looker Studio dashboards for a finance systems consolidation project.
    You now have ACTUAL DATA RESULTS from BigQuery queries, not just SQL analysis.
    
    INPUT DATA:
    - Dashboard ID: {dashboard_id}
    - Dashboard Name: {dashboard_name}
    - Original Analysis: {original_dashboard_analysis}
    - SQL Execution Results: {actual_sql_results}
    - Primary Analysis Data: {primary_analysis_data}
    - Structure Analysis Data: {structure_analysis_data}
    - Validation Results: {validation_results}
    - Business Rules Data: {business_rules_data}
    
    CONSOLIDATION OBJECTIVES:
    1. **METRIC CONSOLIDATION**: Identify duplicate/similar metrics across dashboards
    2. **DATA SOURCE UNIFICATION**: Map to consolidated finance data model
    3. **TRANSFORMATION MAPPING**: Create specific transformation rules
    4. **TESTING STRATEGY**: Design validation approaches for migration
    5. **RELATIONSHIP MODELING**: Design new unified data relationships
    6. **MIGRATION PLANNING**: Create step-by-step migration approach
    
    OUTPUT REQUIREMENTS (JSON):
    {{
      "consolidation_analysis": {{
        "dashboard_id": "string",
        "dashboard_name": "string",
        "consolidation_priority": "high|medium|low",
        "migration_complexity": 1-10,
        "data_quality_score": 1-10,
        "consolidation_readiness": "ready|needs_prep|major_rework",
        "estimated_migration_effort_days": number,
        "business_impact_risk": "high|medium|low",
        "dependencies": ["dashboard_id1", "dashboard_id2"],
        "consolidation_opportunities_count": number
      }},
      
      "metrics_consolidation": [
        {{
          "current_metric_name": "string",
          "current_calculation": "string",
          "data_sample_analysis": "what the actual data tells us about this metric",
          "consolidation_target_metric": "proposed unified metric name",
          "consolidation_rationale": "why these should be consolidated",
          "transformation_rule": "specific transformation logic needed",
          "data_validation_rule": "how to test this transformation",
          "business_impact": "high|medium|low",
          "migration_order": number,
          "similar_metrics_across_dashboards": ["metric references from other dashboards"],
          "unified_definition": "standardised business definition",
          "sample_values_analysis": "insights from actual data values",
          "data_quality_issues": ["specific issues found in data"],
          "proposed_new_calculation": "new standardised calculation method"
        }}
      ],
      
      "data_source_mapping": [
        {{
          "current_source": "project.dataset.table",
          "current_usage": "how this source is used",
          "data_sample_insights": "what the actual data structure reveals", 
          "consolidation_target": "proposed new unified table/view",
          "mapping_complexity": 1-10,
          "transformation_type": "direct_map|calculation_required|major_restructure",
          "key_fields_mapping": {{"old_field": "new_field"}},
          "data_quality_concerns": ["issues found in actual data"],
          "migration_prerequisites": ["steps needed before migration"],
          "testing_approach": "how to validate this mapping"
        }}
      ],
      
      "relationship_model": {{
        "current_relationships": ["how data currently connects"],
        "proposed_unified_model": "description of new consolidated model",
        "key_entities": ["primary business objects"],
        "relationship_changes": ["what relationships need to change"],
        "consolidation_benefits": ["benefits of new model"],
        "implementation_challenges": ["challenges to address"],
        "data_lineage_impact": "how this affects data flow"
      }},
      
      "transformation_specifications": [
        {{
          "transformation_name": "string", 
          "source_logic": "current calculation/logic",
          "target_logic": "new unified logic",
          "transformation_type": "formula_change|aggregation_change|source_change|business_rule_change",
          "sql_transformation": "specific SQL to perform transformation",
          "validation_sql": "SQL to validate transformation worked correctly",
          "rollback_plan": "how to reverse if issues found",
          "testing_data_sample": "sample data to test with",
          "expected_result_range": "expected output values/ranges",
          "business_validation_criteria": "how business users validate correctness"
        }}
      ],
      
      "migration_plan": {{
        "migration_wave": 1-5,
        "migration_order_within_wave": number,
        "prerequisites": ["what must be done first"],
        "migration_steps": ["specific step-by-step actions"],
        "testing_phases": ["validation checkpoints"],
        "rollback_triggers": ["conditions that require rollback"],
        "business_validation_required": ["stakeholder sign-offs needed"],
        "go_live_criteria": ["requirements for production deployment"],
        "post_migration_monitoring": ["what to monitor after migration"]
      }},
      
      "english_summaries": {{
        "dashboard_plain_english": "Simple description of what this dashboard does for business users",
        "consolidation_story": "Plain English explanation of how this fits into consolidation",
        "business_impact_summary": "What happens to users during migration",
        "key_changes_summary": "Main changes users will see",
        "benefits_summary": "Benefits users will gain from consolidation",
        "migration_timeline_summary": "When changes will happen"
      }},
      
      "quality_assessment": {{
        "data_completeness_score": 1-10,
        "data_accuracy_issues": ["problems found in actual data"],
        "calculation_validation_results": ["whether calculations work correctly"],
        "business_logic_issues": ["problems with business rules"],
        "performance_concerns": ["query performance issues"],
        "scalability_issues": ["problems with data volume"],
        "recommendations": ["specific improvements needed"]
      }}
    }}
    
    ANALYSIS REQUIREMENTS USING ACTUAL DATA:
    
    1. **DATA-DRIVEN METRIC ANALYSIS**:
       - Use actual data samples to understand metric behavior
       - Identify metrics that produce similar results (potential duplicates)
       - Analyze data distributions to understand business patterns
       - Find calculation inconsistencies by comparing results
    
    2. **CONSOLIDATION OPPORTUNITY DETECTION**:
       - Compare metrics across dashboards using actual values
       - Identify business logic that can be standardised
       - Find data sources that can be unified
       - Detect redundant calculations
    
    3. **TRANSFORMATION DESIGN**:
       - Create specific transformation rules based on actual data patterns
       - Design validation queries using real data ranges
       - Identify edge cases from actual data samples
       - Plan for data quality improvements
    
    4. **MIGRATION PLANNING**:
       - Sequence migrations based on data dependencies
       - Design testing using actual data samples
       - Plan rollback strategies with real scenarios
       - Create business validation criteria
    
    5. **PLAIN ENGLISH DOCUMENTATION**:
       - Explain dashboards in business terms
       - Describe consolidation benefits clearly
       - Create user-friendly migration communications
       - Write simple testing instructions
    
    CRITICAL ANALYSIS POINTS:
    
    - Use ACTUAL DATA VALUES to inform consolidation decisions
    - Focus on BUSINESS IMPACT and user experience
    - Design TESTABLE transformation rules
    - Create ACTIONABLE migration plans
    - Prioritise based on REAL DATA COMPLEXITY and BUSINESS VALUE
    - Consider DATA QUALITY issues found in actual results
    - Plan for CHANGE MANAGEMENT and user adoption
    
    SAMPLE DATA ANALYSIS APPROACH:
    - Compare metric results across similar dashboards
    - Identify data patterns that indicate consolidation opportunities  
    - Use actual value ranges to design validation rules
    - Analyze data quality issues for migration planning
    - Design transformation logic based on real data behavior
    
    OUTPUT FOCUS:
    Create actionable consolidation guidance that can be directly used for:
    1. Building unified data models
    2. Writing transformation code
    3. Designing test cases
    4. Planning user migration
    5. Communicating changes to stakeholders
    """
    
    return secondary_prompt


def prepare_secondary_batch_input_robust(unified_dataset, df_bq_results):
    """
    Robust version - handles dates, NaNs, and missing data gracefully
    """
    import json
    import pandas as pd
    from datetime import date, datetime
    import numpy as np
    
    print("📊 PREPARING SECONDARY BATCH INPUT (ROBUST VERSION)")
    print("=" * 55)
    
    def clean_for_json(obj):
        """Clean data for JSON serialization"""
        if pd. isna(obj) or obj is None:
            return None
        elif isinstance(obj, (date, datetime)):
            return obj.strftime('%Y-%m-%d') if hasattr(obj, 'strftime') else str(obj)
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj) if not pd.isna(obj) else None
        elif isinstance(obj, dict):
            return {k: clean_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [clean_for_json(item) for item in obj]
        else:
            return obj
    
    # Filter to dashboard summaries only
    dashboard_summaries = unified_dataset[
        unified_dataset['record_type'] == 'dashboard_summary'
    ]
    
    print(f"Processing {len(dashboard_summaries)} dashboard summaries")
    
    secondary_batch_data = []
    combined_results = df_bq_results.get('combined_results', {})
    
    for _, dashboard in dashboard_summaries.iterrows():
        dashboard_id = dashboard['dashboard_id']
        dashboard_name = dashboard.get('dashboard_name', 'Unknown Dashboard')
        
        print(f"  Processing: {dashboard_name[:50]}...")
        
        try:
            # Create original analysis with cleaned data
            original_analysis = {
                'dashboard_id': dashboard_id,
                'dashboard_name': clean_for_json(dashboard.get('dashboard_name')),
                'business_domain': clean_for_json(dashboard.get('business_domain')),
                'complexity_score': clean_for_json(dashboard.get('complexity_score')),
                'consolidation_score': clean_for_json(dashboard.get('consolidation_score')),
                'total_metrics_count': clean_for_json(dashboard.get('total_metrics_count')),
                'kpi_metrics_count': clean_for_json(dashboard.get('kpi_metrics_count')),
                'data_grain': clean_for_json(dashboard.get('data_grain')),
                'date_grain': clean_for_json(dashboard.get('date_grain')),
                'governance_issues_count': clean_for_json(dashboard.get('governance_issues_count'))
            }
            
            # Remove None values
            original_analysis = {k: v for k, v in original_analysis.items() if v is not None}
            
            # Get SQL results - proceed even if some are missing
            actual_results = {}
            
            for query_type in ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']:
                try:
                    if query_type in combined_results and combined_results[query_type] is not None:
                        dashboard_data = combined_results[query_type][
                            combined_results[query_type]['dashboard_id'] == dashboard_id
                        ]
                        
                        if not dashboard_data.empty:
                            # Get a small sample and clean it
                            sample_data = dashboard_data.head(3).copy()
                            
                            # Clean sample data for JSON serialization
                            cleaned_sample = []
                            for _, row in sample_data.iterrows():
                                cleaned_row = {}
                                for col, val in row.items():
                                    if col not in ['dashboard_id', 'response_id', 'query_type']:
                                        cleaned_val = clean_for_json(val)
                                        if cleaned_val is not None:
                                            cleaned_row[col] = cleaned_val
                                if cleaned_row:  # Only add if we have some data
                                    cleaned_sample.append(cleaned_row)
                            
                            actual_results[query_type] = {
                                'status': 'success',
                                'row_count': len(dashboard_data),
                                'columns': [col for col in dashboard_data.columns 
                                          if col not in ['dashboard_id', 'response_id', 'query_type']],
                                'sample_data': cleaned_sample[:2],  # Just 2 rows to keep prompt manageable
                                'data_summary': f"Contains {len(dashboard_data)} rows with business data"
                            }
                        else:
                            actual_results[query_type] = {
                                'status': 'no_data',
                                'message': 'Query executed but returned no rows for this dashboard'
                            }
                    else:
                        actual_results[query_type] = {
                            'status': 'not_available',
                            'message': 'Query not executed or failed - will analyse based on available data'
                        }
                except Exception as e:
                    actual_results[query_type] = {
                        'status': 'error',
                        'message': f'Error processing query results: {str(e)[:100]}'
                    }
            
            # Get metrics for this dashboard
            dashboard_metrics = unified_dataset[
                (unified_dataset['dashboard_id'] == dashboard_id) & 
                (unified_dataset['record_type'] == 'metric')
            ]
            
            metrics_info = []
            for _, metric in dashboard_metrics.iterrows():
                metric_dict = {}
                for key in ['metric_id', 'metric_name', 'metric_type', 'business_description', 
                           'is_kpi', 'calculation_type', 'metric_category']:
                    value = clean_for_json(metric.get(key))
                    if value is not None:
                        metric_dict[key] = value
                
                if len(metric_dict) > 2:  # Only include if we have meaningful data
                    metrics_info.append(metric_dict)
            
            # Create the complete input structure
            secondary_input = {
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard_name,
                'original_dashboard_analysis': original_analysis,
                'dashboard_metrics': metrics_info,
                'actual_sql_results': actual_results,
                'data_availability': {
                    'has_primary_analysis': actual_results.get('primary_analysis_sql', {}).get('status') == 'success',
                    'has_structure_analysis': actual_results.get('structure_sql', {}).get('status') == 'success',
                    'has_validation_results': actual_results.get('validation_sql', {}).get('status') == 'success',
                    'has_business_rules': actual_results.get('business_rules_sql', {}).get('status') == 'success',
                    'metrics_count': len(metrics_info)
                }
            }
            
            # Get the secondary prompt
            secondary_prompt = design_secondary_analysis_prompt()
            
            # Format the prompt - this should work now with cleaned data
            formatted_prompt = secondary_prompt.format(
                dashboard_id=dashboard_id,
                dashboard_name=dashboard_name,
                original_dashboard_analysis=json.dumps(original_analysis, indent=2),
                actual_sql_results=json.dumps(actual_results, indent=2),
                primary_analysis_data=json.dumps(actual_results.get('primary_analysis_sql', {}), indent=2),
                structure_analysis_data=json.dumps(actual_results.get('structure_sql', {}), indent=2),
                validation_results=json.dumps(actual_results.get('validation_sql', {}), indent=2),
                business_rules_data=json.dumps(actual_results.get('business_rules_sql', {}), indent=2)
            )
            
            secondary_batch_data.append({
                'content': formatted_prompt,
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard_name,
                'data_quality': secondary_input['data_availability']
            })
            
            print(f"    ✅ Successfully prepared request")
            
        except Exception as e:
            print(f"    ⚠️ Error preparing {dashboard_id}: {str(e)[:100]}")
            continue
    
    print(f"\n✅ Successfully prepared {len(secondary_batch_data)} secondary analysis requests")
    
    # Show data quality summary
    if secondary_batch_data:
        print(f"\n📊 DATA QUALITY SUMMARY:")
        total_with_primary = sum(1 for req in secondary_batch_data if req['data_quality']['has_primary_analysis'])
        total_with_structure = sum(1 for req in secondary_batch_data if req['data_quality']['has_structure_analysis'])
        total_with_validation = sum(1 for req in secondary_batch_data if req['data_quality']['has_validation_results'])
        
        print(f"  Requests with primary analysis data: {total_with_primary}/{len(secondary_batch_data)}")
        print(f"  Requests with structure analysis data: {total_with_structure}/{len(secondary_batch_data)}")
        print(f"  Requests with validation data: {total_with_validation}/{len(secondary_batch_data)}")
        print(f"  Average metrics per dashboard: {sum(req['data_quality']['metrics_count'] for req in secondary_batch_data) / len(secondary_batch_data):.1f}")
    
    return secondary_batch_data



================================================================================
END OF FILE: functions.py
================================================================================


================================================================================
FILE: requirements.txt
SIZE: 491 characters
================================================================================

# Data manipulation and analysis
pandas>=2.0.0
numpy>=1.24.0

# Google Cloud services
google-cloud-bigquery>=3.11.0
google-cloud-storage>=2.10.0
google-cloud-aiplatform>=1.38.0

# Google authentication and protobuf
google-auth>=2.17.0
protobuf>=4.21.0

# Vertex AI (included in google-cloud-aiplatform but explicit for clarity)
# vertexai is included in google-cloud-aiplatform

# Additional Google Cloud dependencies (usually auto-installed)
google-api-core>=2.11.0
google-cloud-core>=2.3.0

================================================================================
END OF FILE: requirements.txt
================================================================================


================================================================================
FILE: sql/looker_sql.txt
SIZE: 4809 characters
================================================================================

WITH
  -- 1. Identify popular Looker Studio reports (with more than 3 distinct viewers and specific owners)
  LookerStudioPopularReports AS (
    SELECT
      assetId,
      assetTitle,
      assetOwner,
      REPLACE(REPLACE(assetId, 'https://lookerstudio.google.com/reporting/', ''), 'https://datastudio.google.com/reporting/', '') AS report_id_clean
    FROM (
      SELECT
        assetid,
        assettitle,
        assetOwner,
        COUNT(DISTINCT user) AS viewers
      FROM
        `ncau-data-newsquery-prd.cdm_looker_studio.looker_studio_audit_logs`
      GROUP BY
        assetid,
        assettitle,
        assetOwner
    )
    WHERE
      viewers >= 3
      AND assetOwner IN (INSERT) -- <--- Your list of asset owners here
  ),
  -- 2. Identify all unique users who have viewed any of these popular reports.
  UsersOfPopularReports AS (
    SELECT DISTINCT
      LOWER(user) AS username
    FROM
      `ncau-data-newsquery-prd.cdm_looker_studio.looker_studio_audit_logs` lsal
    INNER JOIN
      LookerStudioPopularReports lspr ON lsal.assetId = lspr.assetId
  ),
  -- 3. Get BigQuery audit logs for jobs initiated by the identified users,
  --    and extract the Looker Studio report ID from job labels.
  BigQueryAuditLogsFilteredByUser AS (
    SELECT
      protopayload_auditlog.authenticationInfo.principalEmail AS username,
      receiveTimestamp,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobName.jobId,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.startTime,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.endTime,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalSlotMs,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalProcessedBytes,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalBilledBytes,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.reservation,
      (
        SELECT value
        FROM UNNEST(protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.labels)
        WHERE key = 'looker_studio_report_id'
      ) AS looker_studio_report_id,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.query.query AS query_text
    FROM
      `ncau-data-newsquery-prd.ops_prd_bq_logs.cloudaudit_googleapis_com_data_access`
    WHERE
      DATE(receiveTimestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
      AND protopayload_auditlog.authenticationInfo.principalEmail IN (SELECT username FROM UsersOfPopularReports)
      AND (
            SELECT value
            FROM UNNEST(protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.labels)
            WHERE key = 'looker_studio_report_id'
          ) IS NOT NULL
  ),
  -- 4. Join BigQuery jobs with popular report info and rank jobs randomly per report
  RankedBigQueryJobsPerReport AS (
    SELECT
      bqa.jobId,
      bqa.username,
      lspr.assetTitle AS looker_studio_report_name,
      lspr.assetOwner,
      bqa.looker_studio_report_id,
      bqa.startTime,
      bqa.endTime,
      bqa.receiveTimestamp,
      DATE_DIFF(bqa.endTime, bqa.startTime, SECOND) AS runtime_seconds,
      bqa.totalSlotMs,
      bqa.totalProcessedBytes,
      bqa.totalBilledBytes,
      bqa.reservation,
      bqa.query_text,
      -- Use a random ordering for ROW_NUMBER to get 'different flavors'
      ROW_NUMBER() OVER (PARTITION BY bqa.looker_studio_report_id ORDER BY FARM_FINGERPRINT(bqa.jobId)) AS rn
      -- Or, for a time-based random sample (less truly random but still varies):
      -- ROW_NUMBER() OVER (PARTITION BY bqa.looker_studio_report_id ORDER BY RAND()) AS rn -- RAND() is non-deterministic
      -- Better for consistent random: ORDER BY FARM_FINGERPRINT(CONCAT(bqa.jobId, CAST(bqa.receiveTimestamp AS STRING)))
    FROM
      BigQueryAuditLogsFilteredByUser bqa
    INNER JOIN
      LookerStudioPopularReports lspr
    ON
      bqa.looker_studio_report_id = lspr.report_id_clean
  )
-- 5. Final Selection: Retrieve only the top N (e.g., 3) randomly sampled jobs for each popular report
SELECT
  jobId,
  username,
  looker_studio_report_name,
  assetOwner,
  looker_studio_report_id,
  startTime,
  endTime,
  runtime_seconds,
  totalSlotMs,
  totalProcessedBytes,
  totalBilledBytes,
  reservation,
  query_text
FROM
  RankedBigQueryJobsPerReport
WHERE
  rn <= 3 -- Select only the top 3 rows per report (randomly picked)
ORDER BY
  looker_studio_report_name,
  jobId; -- Order by report name and then jobId for consistent output

================================================================================
END OF FILE: sql/looker_sql.txt
================================================================================
