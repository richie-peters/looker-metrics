
PROJECT CODE COMPILATION
Generated: Wed Jul  2 07:26:54 AM UTC 2025
Directory: /content/looker-metrics
Files processed: 6

================================================================================
TABLE OF CONTENTS
================================================================================

================================================================================
FILE: config.py
SIZE: 5833 characters
================================================================================

"""
Configuration and Setup Module (EMAIL_LIST FIXED)
=================================================

This module handles environment setup, authentication, and project configurations.

Author: Data Team
Date: 2025-01-07
"""

import pandas as pd
import os
import sys
from datetime import datetime
import warnings
from google.cloud import bigquery, storage
import vertexai
from google.auth import default

# Project-specific configurations
BQ_PROJECT_ID = "ncau-data-newsquery-prd"
VERTEX_PROJECT_ID = "ncau-data-nprod-aitrain"
REGION = "us-central1"

# GCS Paths
INPUT_GCS_URI = "gs://looker_metrics/input.jsonl"
OUTPUT_GCS_URI = "gs://looker_metrics/output/"

# Gemini Model Configuration
GEMINI_MODEL_NAME = "gemini-2.5-flash"
MAX_OUTPUT_TOKENS = 65000
TEMPERATURE = 0.20

# SQL File Path
LOOKER_SQL_FILE = "./sql/looker_sql.txt"

# Email List for SQL Queries
EMAIL_LIST = [
    'santhosh.kanaparthi@news.com.au','desnica.kumar@news.com.au','romy.li@news.com.au','cecile.desphy@news.com.au','jeyaram.jawahar@news.com.au','nigel.aye@news.com.au','camille.shi@news.com.au','justin.guo@news.com.au','eric.loi@news.com.au','pravarthika.rathinakumar@news.com.au','kylie.lu@news.com.au','ritwik.deo@news.com.au','harry.mcauley@news.com.au'
]

# Directory Structure
BASE_PATH = "."
DIRECTORIES = {
    'functions': os.path.join(BASE_PATH, 'functions'),
    'scripts': os.path.join(BASE_PATH, 'scripts'),
    'data': os.path.join(BASE_PATH, 'data'),
    'sql': os.path.join(BASE_PATH, 'sql'),
    'results': os.path.join(BASE_PATH, 'results'),
    'logs': os.path.join(BASE_PATH, 'logs')
}

def setup_environment():
    """Configure global environment settings for analysis."""
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 50)
    pd.set_option('display.float_format', lambda x: '%.2f' % x)
    warnings.filterwarnings('ignore')
    print("âœ“ Environment configured successfully")

def create_directory_structure(base_path=BASE_PATH):
    """Create standard directory structure for the project."""
    for name, path in DIRECTORIES.items():
        os.makedirs(path, exist_ok=True)
        print(f"âœ“ Directory ready: {name} -> {path}")
    return DIRECTORIES

def validate_project_setup(bq_project_id=BQ_PROJECT_ID, vertex_project_id=VERTEX_PROJECT_ID, region=REGION):
    """Validate that GCP projects and services are properly configured."""
    validation_results = {
        'bigquery': False,
        'vertex_ai': False,
        'storage': False,
        'authentication': False,
        'projects_valid': False
    }

    try:
        bq_client = bigquery.Client(project=bq_project_id)
        bq_client.query("SELECT 1 as test").result()
        validation_results['bigquery'] = True
        print(f"âœ“ BigQuery access validated: {bq_project_id}")
    except Exception as e:
        print(f"âœ— BigQuery validation failed: {str(e)}")

    try:
        vertexai.init(project=vertex_project_id, location=region)
        from vertexai.preview import generative_models
        model = generative_models.GenerativeModel("gemini-pro")
        validation_results['vertex_ai'] = True
        print(f"âœ“ Vertex AI access validated: {vertex_project_id}")
    except Exception as e:
        print(f"âœ— Vertex AI validation failed: {str(e)}")

    try:
        storage_client = storage.Client()
        validation_results['storage'] = True
        print(f"âœ“ Cloud Storage access validated")
    except Exception as e:
        print(f"âœ— Cloud Storage validation failed: {str(e)}")

    try:
        credentials, project = default()
        validation_results['authentication'] = True
        validation_results['default_project'] = project
        print(f"âœ“ Authentication validated: {project}")
    except Exception as e:
        print(f"âœ— Authentication validation failed: {str(e)}")

    validation_results['projects_valid'] = all(validation_results[key] for key in ['bigquery', 'vertex_ai', 'storage', 'authentication'])

    if validation_results['projects_valid']:
        print(f"âœ“ All project validations passed")
    else:
        print(f"âœ— Some validations failed - check configuration")

    return validation_results

def initialize_session():
    """Initialise a complete analysis session."""
    print("=" * 60)
    print("INITIALIZING LOOKER ANALYSIS SESSION")
    print("=" * 60)

    setup_environment()
    directories = create_directory_structure()
    validation_results = validate_project_setup()

    session_config = {
        'session_id': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'start_time': datetime.now(),
        'directories': directories,
        'project_config': {
            'bq_project': BQ_PROJECT_ID,
            'vertex_project': VERTEX_PROJECT_ID,
            'region': REGION
        },
        'validation_results': validation_results
    }

    print("\n" + "=" * 60)
    print("âœ“ ANALYSIS SESSION INITIALIZED")
    print("=" * 60)
    print(f"Session ID: {session_config['session_id']}")
    print(f"Start time: {session_config['start_time']}")
    print(f"BigQuery Project: {session_config['project_config']['bq_project']}")
    print(f"Vertex AI Project: {session_config['project_config']['vertex_project']}")

    return session_config



def print_session_summary(session_config):
    """Print a summary of the current session configuration."""
    print("\n" + "=" * 60)
    print("SESSION SUMMARY")
    print("=" * 60)

    print(f"BigQuery Project: {session_config['project_config']['bq_project']}")
    print(f"Vertex AI Project: {session_config['project_config']['vertex_project']}")
    print(f"Region: {session_config['project_config']['region']}")
    print(f"Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Working Directory: {os.getcwd()}")
    print("\n" + "=" * 60)


================================================================================
END OF FILE: config.py
================================================================================


================================================================================
FILE: README.md
SIZE: 3251 characters
================================================================================

# Looker Studio Analysis Pipeline

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-1e3a5f?style=for-the-badge&logo=python&logoColor=f5f5dc)
![Google Cloud](https://img.shields.io/badge/Google_Cloud-BigQuery_|_Vertex_AI-1e3a5f?style=for-the-badge&logo=google-cloud&logoColor=f5f5dc)
![Colab](https://img.shields.io/badge/Google-Colab-1e3a5f?style=for-the-badge&logo=google-colab&logoColor=f5f5dc)

**Automated pipeline for analysing Looker Studio metrics using BigQuery and Gemini AI**

</div>

<br>

## Overview

Enterprise-grade pipeline that extracts data from BigQuery and processes it using Gemini AI for intelligent Looker Studio analytics.

<br>

## Features

- **BigQuery Integration** - Automated data extraction with SQL templates
- **AI Processing** - Gemini-powered analysis and insights  
- **Batch Operations** - Scalable processing for large datasets
- **CSV Export** - Structured output for further analysis

<br>

## Installation & Execution

### Step 1: Setup Repository
```python
# Clone and navigate to project
!git clone https://github.com/richie-peters/looker-metrics.git
import os
os.chdir('/content/looker-metrics')
Step 2: Install Dependencies# Install required packages
!pip install -q -r requirements.txt
Step 3: Authenticate# Google Cloud authentication
from google.colab import auth
auth.authenticate_user()
Step 4: ConfigureUpdate config.py with your settings:
BQ_PROJECT_ID - Your BigQuery project
VERTEX_PROJECT - Your Vertex AI project
INPUT_GCS_URI - Input bucket path
OUTPUT_GCS_URI - Output bucket path
Step 5: Run Pipeline# Execute the analysis
exec(open('main.py').read())
<br>ConfigurationParameterDescriptionExampleBQ_PROJECT_IDBigQuery project ID"my-project"VERTEX_PROJECTVertex AI project ID"my-vertex-project"INPUT_GCS_URIInput storage location"gs://bucket/input/"OUTPUT_GCS_URIOutput storage location"gs://bucket/output/"EMAIL_LISTTarget emails for analysisList of email strings<br>Project Structurelooker-metrics/
â”œâ”€â”€ main.py              # Main execution script
â”œâ”€â”€ config.py            # Configuration settings  
â”œâ”€â”€ functions.py         # Core functions
â”œâ”€â”€ sql/                 # SQL query templates
â”œâ”€â”€ data/                # Output directory
â””â”€â”€ requirements.txt     # Dependencies
<br>TroubleshootingAuthentication Issuesfrom google.colab import auth
auth.authenticate_user()
No Data Returned
Check BigQuery permissions
Verify email list configuration
Confirm dataset exists
Processing Errors
Enable Vertex AI API
Validate GCS bucket access
Check model availability
Debug Results# Check processing output
if 'global_results' in globals():
    print(f"Processed: {len(global_results)} records")
<br>Output
CSV Files - Generated in ./data/ directory
Console Logs - Execution summary and statistics
Global Variables - Results stored in global_results
<br>CustomisationEmail List: Edit EMAIL_LIST in main.pyEMAIL_LIST = ['user@company.com', 'user2@company.com']
Processing: Modify parameters in config.py and functions.py<br>Support
Review documentation and troubleshooting section
Check execution logs for error details
Contact Data Team for assistance
Submit GitHub issues for bugs
<br><div align="center">Enterprise Data Pipeline SolutionData Team | Production Ready</div>
```

================================================================================
END OF FILE: README.md
================================================================================


================================================================================
FILE: main.py
SIZE: 1965 characters
================================================================================

"""
Looker Studio Analysis Pipeline - Direct Execution
==================================================
"""

import pandas as pd
import config
import functions

# Configuration
EMAIL_LIST = ["richie.peters@news.com.au"]
global_results = None

# 1. Initialise session
session_config = config.initialize_session()
config.print_session_summary(session_config)

# 2. Extract data from BigQuery
print("\n--- Extracting data from BigQuery ---")
replacements = {'INSERT': functions.format_emails_for_sql(EMAIL_LIST)}
query_df = functions.run_sql_file(config.LOOKER_SQL_FILE, replacements=replacements, project=config.BQ_PROJECT_ID)

if query_df is None or query_df.empty:
    print("âœ— No data extracted from BigQuery. Exiting.")
else:
    # 3. Prepare batch input for Gemini
    print("\n--- Preparing batch input for Gemini ---")
    batch_data = functions.prepare_looker_analysis_batch(query_df)

    # 4. Run Gemini batch prediction (SLICK VERSION)
    print("\n--- Running Gemini batch prediction ---")
    results = functions.run_gemini_batch_fast_slick(
        requests=batch_data,
        project=session_config['project_config']['vertex_project'],
        display_name="looker-analysis-batch",
        input_gcs_uri=config.INPUT_GCS_URI,
        output_gcs_uri=config.OUTPUT_GCS_URI,
        model_name=config.GEMINI_MODEL_NAME
    )

    if results is None:
        print("âœ— Gemini batch prediction failed. Exiting.")
    else:
        # Make results global for troubleshooting
        global_results = results
        print(f"âœ“ Results stored in global_results variable ({len(results)} items)")

        # 5. Process and save results
        print("\n--- Processing results ---")
        datasets = functions.convert_batch_results_to_dataset(results)
        if datasets:
            functions.save_datasets_to_csv(datasets, "./data/")
            functions.analyse_results_summary(datasets)

        print(f"âœ“ Successfully processed {len(results)} results.")

================================================================================
END OF FILE: main.py
================================================================================


================================================================================
FILE: functions.py
SIZE: 97709 characters
================================================================================

"""
Consolidated Functions Module (With Looker Analysis Prompt)
===========================================================

This module contains all custom functions for the Looker Studio analysis pipeline,
including SQL utilities, batch processing utilities, and data processing utilities.

Author: Data Team
Date: 2025-01-07
"""

import json
import time
import os
import sys
from datetime import datetime
from google.cloud import bigquery, storage
from google.cloud import aiplatform_v1beta1, aiplatform_v1
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
import vertexai
from vertexai.generative_models import GenerativeModel
import pandas as pd
import numpy as np

# Looker Analysis Prompt
LOOKER_ANALYSIS_PROMPT = """
Analyze these Looker Studio dashboard SQL queries and extract comprehensive metrics information with unified dataset analysis.

INPUT DATA:
- Dashboard ID: {dashboard_id}
- Dashboard Name: {dashboard_name}
- SQL Samples: {sql_samples}

ANALYSIS REQUIREMENTS:

1. **METRIC EXTRACTION**: Identify all business metrics, dimensions, and calculations
2. **SQL DECOMPOSITION**: Break down complex nested queries into logical components
3. **DEPENDENCY MAPPING**: Identify which calculations depend on others
4. **BUSINESS LOGIC**: Extract the core business rules and transformations
5. **DATASET STRUCTURE**: Understand data grain, key dimensions, and metric interactions
6. **HARDCODED VALUES**: Identify hardcoded dates, values, and variables that should be parameterised or use governed tables
7. **UNIFIED ANALYSIS**: Create consolidated queries that analyze all metrics together
8. **STANDARDISATION**: Use standardised classes and values for consistent analysis
9. **BIGQUERY COMPLIANCE**: Generate only valid BigQuery SQL syntax

OUTPUT REQUIREMENTS (JSON):
Return a flat JSON structure with the following schema:

{{
  "dashboard_summary": {{
    "dashboard_id": "string",
    "dashboard_name": "string",
    "primary_data_sources": "project1.dataset1.table1;project2.dataset2.table2;project3.dataset3.table3",
    "business_domain": "advertising|finance|consumer|operations|marketing|sales|product|hr|other",
    "complexity_score": 1-10,
    "consolidation_score": 1-10,
    "total_metrics_identified": number,
    "date_grain": "daily|weekly|monthly|quarterly|yearly|mixed|none",
    "data_grain": "transactional|aggregate",
    "key_dimensions": ["date", "customer_id", "product", "region"],
    "date_range_detected": "01/01/2024 to Current",
    "hardcoded_dates_found": ["03/07/2024", "30/03/2025", "01/01/2024"],
    "hardcoded_values_found": ["specific product IDs", "region codes", "business unit names"],
    "governance_opportunities": ["dates should be parameterised", "product codes should join to product_master", "regions should use location_hierarchy"]
  }},
  "dataset_analysis": {{
    "primary_analysis_sql": "**THIS IS THE MAIN SQL TO RUN** - Single query showing all key metrics calculated together with appropriate sampling and date filters",
    "structure_sql": "Query to understand data structure, grain, and key dimensions with sampling",
    "validation_sql": "Quick validation that all metric calculations work syntactically",
    "business_rules_sql": "Query to validate key business logic, filters, and data quality",
    "sample_data_sql": "Query to get representative sample data for further analysis",
    "hardcoded_issues": {{
      "hardcoded_dates": [
        {{
          "date_value": "03/07/2024",
          "original_format": "2024-07-03T00:00:00",
          "context": "used as baseline date in DATETIME_DIFF calculation",
          "suggested_fix": "replace with CURRENT_DATE() or parameter",
          "impact": "high|medium|low",
          "urgency": "high|medium|low"
        }}
      ],
      "hardcoded_variables": [
        {{
          "variable_type": "lookup_codes|business_rules|thresholds|categories|other",
          "hardcoded_values": ["'TA'", "'DT'", "'HS'"],
          "context": "masthead codes hardcoded in CASE statement",
          "suggested_governance": "join to masthead_lookup table",
          "impact": "high|medium|low",
          "maintenance_risk": "high|medium|low"
        }}
      ]
    }},
    "parameterisation_recommendations": [
      "Replace hardcoded dates with date parameters or relative date functions",
      "Replace hardcoded lookup values with joins to governed reference tables",
      "Use configuration tables for business rules instead of hardcoded logic"
    ]
  }},
  "metrics": [
    {{
      "metric_id": "unique_identifier_snake_case",
      "metric_name": "Human Readable Name",
      "metric_type": "dimension|measure|calculated_field|filter|aggregation|ratio|percentage",
      "calculation_type": "sum|count|count_distinct|average|min|max|ratio|case_when|date_function|string_function|mathematical|conditional",
      "data_type": "numeric|string|date|boolean|array",
      "aggregation_level": "transaction|daily|weekly|monthly|quarterly|yearly|customer|product|region|custom",
      "is_final_output": true|false,
      "is_kpi": true|false,
      "business_criticality": "high|medium|low",
      "depends_on_metrics": ["metric_id1", "metric_id2"],
      "business_description": "what this metric represents in business terms",
      "sql_logic": "core SQL calculation logic extracted from queries",
      "data_sources": ["project.dataset.table1", "project.dataset.table2"],
      "filters_applied": ["date filters", "business rules", "exclusions"],
      "expected_data_type": "integer|decimal|string|date|boolean",
      "business_context": "how this metric fits into overall business analysis",
      "metric_category": "revenue|cost|volume|efficiency|quality|growth|retention|acquisition|other",
      "update_frequency": "real_time|hourly|daily|weekly|monthly|quarterly|yearly|on_demand",
      "seasonality_impact": "high|medium|low|none",
      "hardcoded_dates_in_metric": ["03/07/2024", "01/01/2025"],
      "hardcoded_values_in_metric": ["'Metro'", "'Regional'", "'TA'", "'DT'"],
      "governance_issues": ["date should be parameterised", "lookup values should use reference table"],
      "data_quality_concerns": ["potential nulls", "outliers expected", "data freshness dependent"]
    }}
  ],
  "metric_interactions": [
    {{
      "interaction_type": "mathematical_relationship|dependency|filter_impact|hierarchical|causal",
      "primary_metric": "metric_id",
      "related_metrics": ["metric_id1", "metric_id2"],
      "relationship_description": "how these metrics relate to each other",
      "mathematical_formula": "if applicable: A = B * C or A = B + C",
      "business_validation": "what this relationship means for business analysis",
      "validation_sql": "SQL to test this relationship holds true",
      "relationship_strength": "strong|medium|weak",
      "business_impact": "high|medium|low"
    }}
  ]
}}

CRITICAL BIGQUERY SQL REQUIREMENTS:

**NEVER DO THESE - THEY CAUSE FAILURES:**
âŒ Compare INT64 with STRING: WHERE fiscal_week_id = 'CP'
âŒ Use LPAD on INT64: LPAD(week_number, 2, '0')
âŒ Create arrays with NULLs: ARRAY[col1, col2, null_col]
âŒ Missing GROUP BY: SELECT customer, revenue FROM table GROUP BY customer
âŒ Compare different types: WHERE year_column = 'CP' (if year_column is INT64)

**ALWAYS DO THESE - THEY WORK:**
âœ… Cast before comparing: WHERE CAST(fiscal_week_id AS STRING) = 'CP' OR WHERE fiscal_week_id = CAST('2024' AS INT64)
âœ… Cast before string functions: LPAD(CAST(week_number AS STRING), 2, '0')
âœ… Handle NULLs in arrays: ARRAY(SELECT x FROM UNNEST([col1, col2, col3]) AS x WHERE x IS NOT NULL)
âœ… Aggregate or group all columns: SELECT customer, SUM(revenue) as total_revenue FROM table GROUP BY customer
âœ… Use SAFE_CAST for safety: WHERE SAFE_CAST(column AS STRING) = 'value'

DATASET ANALYSIS REQUIREMENTS (BIGQUERY-COMPLIANT):

1. **primary_analysis_sql**: **THE MAIN QUERY TO EXECUTE**
   ```sql
   -- Example structure - MUST be valid BigQuery syntax:
   WITH base_data AS (
     SELECT 
       date_dimension,
       primary_grouping_dimension,
       revenue_column,
       customer_column
     FROM `project.dataset.table`
     WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
   )
   SELECT 
     date_dimension,
     primary_grouping_dimension,
     COUNT(*) as record_count,
     SUM(SAFE_CAST(revenue_column AS NUMERIC)) as total_revenue,
     COUNT(DISTINCT customer_column) as unique_customers,
     AVG(SAFE_CAST(revenue_column AS NUMERIC)) as avg_revenue_per_record
   FROM base_data
   GROUP BY date_dimension, primary_grouping_dimension  -- MUST include all non-aggregated columns
   ORDER BY date_dimension DESC
   LIMIT 100


structure_sql: Understand data structure - HANDLE TYPE MISMATCHES
SELECT 
  'Data Structure Analysis' as analysis_type,
  COUNT(*) as total_records,
  COUNT(DISTINCT SAFE_CAST(date_column AS DATE)) as unique_dates,
  COUNT(DISTINCT customer_column) as unique_customers,
  COUNT(DISTINCT SAFE_CAST(fiscal_week_id AS STRING)) as unique_fiscal_weeks,
  MIN(SAFE_CAST(date_column AS DATE)) as earliest_date,
  MAX(SAFE_CAST(date_column AS DATE)) as latest_date,
  APPROX_COUNT_DISTINCT(primary_key) as approx_unique_records
FROM `project.dataset.main_table` 
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)



validation_sql: Quick validation - USE SAFE_CAST
SELECT 
  'Validation Check' as test_type,
  CASE WHEN SUM(SAFE_CAST(revenue AS NUMERIC)) > 0 THEN 'PASS' ELSE 'FAIL' END as revenue_test,
  CASE WHEN COUNT(DISTINCT customer_column) > 0 THEN 'PASS' ELSE 'FAIL' END as customer_test,
  CASE WHEN MAX(SAFE_CAST(date_column AS DATE)) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) THEN 'PASS' ELSE 'FAIL' END as freshness_test,
  CASE WHEN COUNT(CASE WHEN SAFE_CAST(status_column AS STRING) IN ('CP', 'PY') THEN 1 END) > 0 THEN 'PASS' ELSE 'FAIL' END as status_test
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
LIMIT 1



business_rules_sql: Business logic validation - HANDLE ARRAYS PROPERLY
SELECT 
  'Business Rule Validation' as validation_type,
  'period_type_validation' as rule_name,
  COUNT(*) as records_tested,
  SUM(CASE WHEN SAFE_CAST(period_column AS STRING) IN ('CP', 'PY') THEN 1 ELSE 0 END) as records_passing_rule,
  SAFE_DIVIDE(SUM(CASE WHEN SAFE_CAST(period_column AS STRING) IN ('CP', 'PY') THEN 1 ELSE 0 END), COUNT(*)) * 100 as pass_rate_percentage
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)



sample_data_sql: Representative sample - CAST DATE COLUMNS
SELECT 
  -- Cast all potentially problematic columns
  SAFE_CAST(date_column AS DATE) as date_column,
  customer_dimension,
  product_dimension,
  SAFE_CAST(fiscal_week_id AS STRING) as fiscal_week_id,
  SAFE_CAST(revenue_metric AS NUMERIC) as revenue_metric,
  SAFE_CAST(volume_metric AS NUMERIC) as volume_metric
FROM `project.dataset.main_table`
WHERE SAFE_CAST(date_column AS DATE) >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY)
  AND revenue_metric IS NOT NULL
ORDER BY SAFE_CAST(date_column AS DATE) DESC, SAFE_CAST(revenue_metric AS NUMERIC) DESC
LIMIT 500


BIGQUERY SYNTAX ENFORCEMENT:

Type Casting Rules:

ALWAYS use SAFE_CAST() instead of implicit conversion
Cast before any comparison: SAFE_CAST(column AS STRING) = 'value'
Cast before string functions: LPAD(CAST(number AS STRING), 2, '0')



Aggregation Rules:

Every non-aggregated column in SELECT must be in GROUP BY
Use SUM, COUNT, AVG, MAX, MIN for calculated fields
When unsure, aggregate the column: SUM(column) or MAX(column)



Array Handling:

Never include NULL in arrays: ARRAY(SELECT x FROM UNNEST([col1, col2]) AS x WHERE x IS NOT NULL)
Use ARRAY_AGG() for creating arrays from query results



Date/Time Functions:

Use CURRENT_DATE() not CURRENT_DATE
Use DATE_SUB(CURRENT_DATE(), INTERVAL n DAY)
Cast date columns: SAFE_CAST(date_col AS DATE)



String Comparisons:

Always cast numbers to strings before string comparison
Use SAFE_CAST to avoid errors: SAFE_CAST(fiscal_week AS STRING) = 'CP'


IMPORTANT NOTES:
primary_analysis_sql is the main SQL to execute - this gives you dashboard metrics with real data
All SQL must be valid BigQuery syntax - test every comparison and function
Use SAFE_CAST extensively - prevents type mismatch errors
Always GROUP BY non-aggregated columns - prevents grouping errors
Handle NULLs in arrays explicitly - prevents array errors
Focus on business logic while ensuring technical correctness
Prioritise metrics marked as is_kpi=true and business_criticality=high
All SQL queries should use appropriate sampling and be cost-optimised
"""



# SQL Utilities
def read_and_replace_sql(file_path, replacements=None):
    """Read SQL file and replace specified text patterns."""
    if replacements is None:
        replacements = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            sql = f.read()
        for old_text, new_text in replacements.items():
            sql = sql.replace(old_text, new_text)
        return sql
    except FileNotFoundError:
        raise FileNotFoundError(f"SQL file not found: {file_path}")
    except Exception as e:
        raise Exception(f"Error reading SQL file {file_path}: {str(e)}")

def execute_bq_query(sql, client=None, project=None, to_dataframe=True):
    """Execute SQL query in BigQuery."""
    if client is None:
        client = bigquery.Client(project=project)
    try:
        if to_dataframe:
            df = client.query(sql).to_dataframe()
            print(f"âœ“ Query executed successfully - returned {len(df)} rows")
            return df
        else:
            job = client.query(sql)
            job.result()
            print(f"âœ“ Query executed successfully")
            return job
    except Exception as e:
        print(f"âœ— Query execution failed: {str(e)}")
        return None

def run_sql_file(file_path, replacements=None, client=None, project=None):
    """Read SQL file, apply replacements, and execute in BigQuery."""
    if replacements is None:
        replacements = {}
    try:
        sql = read_and_replace_sql(file_path, replacements)
        return execute_bq_query(sql, client, project)
    except Exception as e:
        print(f"âœ— Failed to run SQL file {file_path}: {str(e)}")
        return None

def format_emails_for_sql(email_list):
    """Format email list for use in SQL IN clauses."""
    if not email_list:
        return "''"
    formatted_emails = "', '".join(email_list)
    return f"'{formatted_emails}'"

# Vertex AI Batch Processing Utilities
def prepare_batch_input_for_gemini(requests, output_gcs_path, temperature=0.20, max_output_tokens=65000):
    """Prepare batch input in correct format for Gemini models."""
    try:
        batch_requests = []
        for i, request in enumerate(requests):
            batch_requests.append({
                "request": {
                    "contents": [
                        {
                            "role": "user",
                            "parts": [{"text": request["content"]}]
                        }
                    ],
                    "generation_config": {
                        "temperature": temperature,
                        "max_output_tokens": max_output_tokens
                    }
                }
            })
        client = storage.Client()
        bucket_name = output_gcs_path.replace('gs://', '').split('/')[0]
        blob_path = '/'.join(output_gcs_path.replace('gs://', '').split('/')[1:])
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        jsonl_content = ""
        for request in batch_requests:
            jsonl_content += json.dumps(request) + "\n"
        blob.upload_from_string(jsonl_content, content_type='application/json')
        print(f"âœ“ Batch input uploaded to {output_gcs_path}")
        print(f"  Records: {len(batch_requests)}")
        print(f"  Temperature: {temperature}, Max tokens: {max_output_tokens}")
        return True
    except Exception as e:
        print(f"âœ— Failed to prepare batch input: {str(e)}")
        return False

def create_vertex_batch_prediction_job(project, display_name, model_name,
                                      input_gcs_uri, output_gcs_uri,
                                      location="us-central1",
                                      instances_format="jsonl",
                                      predictions_format="jsonl"):
    """Create batch prediction job using Vertex AI v1beta1 API."""
    try:
        api_endpoint = f"{location}-aiplatform.googleapis.com"
        client_options = {"api_endpoint": api_endpoint}
        client = aiplatform_v1beta1.JobServiceClient(client_options=client_options)
        batch_prediction_job = {
            "display_name": display_name,
            "model": model_name,
            "input_config": {
                "instances_format": instances_format,
                "gcs_source": {"uris": [input_gcs_uri]},
            },
            "output_config": {
                "predictions_format": predictions_format,
                "gcs_destination": {"output_uri_prefix": output_gcs_uri},
            }
        }
        parent = f"projects/{project}/locations/{location}"
        response = client.create_batch_prediction_job(
            parent=parent, batch_prediction_job=batch_prediction_job
        )
        print(f"âœ“ Batch prediction job created: {display_name}")
        print(f"  Job name: {response.name}")
        print(f"  Model: {model_name}")
        print(f"  Input: {input_gcs_uri}")
        print(f"  Output: {output_gcs_uri}")
        return response
    except Exception as e:
        print(f"âœ— Batch prediction job creation failed: {str(e)}")
        return None

def monitor_batch_prediction_job_quiet(job_name, location="us-central1"):
    """Monitor batch prediction job status - minimal output"""
    try:
        api_endpoint = f"{location}-aiplatform.googleapis.com"
        client_options = {"api_endpoint": api_endpoint}
        client = aiplatform_v1beta1.JobServiceClient(client_options=client_options)
        
        job = client.get_batch_prediction_job(name=job_name)
        
        if hasattr(job.state, 'name'):
            state_name = job.state.name
        else:
            state_name = str(job.state)
        
        return {
            'name': job.name,
            'display_name': job.display_name,
            'state': state_name,
            'create_time': job.create_time,
            'update_time': job.update_time,
            'error': job.error if hasattr(job, 'error') else None
        }
        
    except Exception as e:
        print(f"âœ— Failed to get job status: {str(e)}")
        return None

def wait_for_batch_completion_slick(job_name, location="us-central1", check_interval=10, max_wait=7200):
    """Wait for batch prediction job with slick progress display"""
    print(f"Monitoring batch job: {job_name.split('/')[-1]}")
    start_time = time.time()
    
    completion_states = ['JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED']
    success_states = ['JOB_STATE_SUCCEEDED']
    
    while time.time() - start_time < max_wait:
        status = monitor_batch_prediction_job_quiet(job_name, location)
        
        if status:
            state = status['state']
            elapsed = int(time.time() - start_time)
            elapsed_str = f"{elapsed//60}m {elapsed%60}s"
            
            # Create status line
            if state == 'JOB_STATE_PENDING':
                status_line = f"â³ PENDING | Elapsed: {elapsed_str}"
            elif state == 'JOB_STATE_RUNNING':
                status_line = f"ðŸƒ RUNNING | Elapsed: {elapsed_str}"
            else:
                status_line = f"ðŸ“Š {state} | Elapsed: {elapsed_str}"
            
            # Overwrite previous line
            sys.stdout.write(f"\r{status_line}")
            sys.stdout.flush()
            
            # Check if completed
            if state in completion_states:
                sys.stdout.write("\n")  # New line after completion
                if state in success_states:
                    print(f"âœ“ Job completed successfully!")
                else:
                    print(f"âœ— Job completed with status: {state}")
                return status
        
        time.sleep(check_interval)
    
    sys.stdout.write("\n")
    print(f"âœ— Job did not complete within {max_wait} seconds")
    return None

def read_batch_prediction_results_fixed(output_gcs_prefix):
    """Read batch prediction results from GCS - fixed version"""
    try:
        client = storage.Client()
        bucket_name = output_gcs_prefix.replace('gs://', '').split('/')[0]
        prefix = '/'.join(output_gcs_prefix.replace('gs://', '').split('/')[1:]).rstrip('/')
        
        print(f"Searching for results in gs://{bucket_name}/{prefix}")
        
        bucket = client.bucket(bucket_name)
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        # Find the main predictions.jsonl files (not incremental ones)
        prediction_files = [blob for blob in blobs 
                          if blob.name.endswith('predictions.jsonl') 
                          and 'incremental' not in blob.name 
                          and blob.size > 1000]
        
        if not prediction_files:
            print("No main predictions.jsonl files found!")
            return []
        
        # Sort by creation time and get the most recent
        prediction_files.sort(key=lambda x: x.time_created, reverse=True)
        
        results = []
        blob = prediction_files[0]
        print(f"Reading: {blob.name}")
        content = blob.download_as_text()
        
        for line_num, line in enumerate(content.strip().split('\n')):
            if line.strip():
                try:
                    result = json.loads(line)
                    results.append(result)
                except json.JSONDecodeError as e:
                    print(f"Error parsing line {line_num}: {e}")
        
        print(f"âœ“ Successfully read {len(results)} predictions")
        return results
        
    except Exception as e:
        print(f"âœ— Failed to read batch results: {e}")
        return []

def run_gemini_batch_fast_slick(
    requests,
    project,
    display_name,
    input_gcs_uri,
    output_gcs_uri,
    model_name="gemini-1.5-flash",
    location="us-central1",
    max_output_tokens=65000,
    temperature=0.20
):
    """Run complete Gemini batch prediction workflow - slick version"""
    
    # Step 1: Prepare and upload input
    print("ðŸ“¤ Preparing batch input...")
    success = prepare_batch_input_for_gemini(requests, input_gcs_uri, temperature, max_output_tokens)
    if not success:
        return None
    
    # Step 2: Create batch job
    print("ðŸš€ Creating batch prediction job...")
    job = create_vertex_batch_prediction_job(
        project=project,
        display_name=display_name,
        model_name=f"publishers/google/models/{model_name}",
        input_gcs_uri=input_gcs_uri,
        output_gcs_uri=output_gcs_uri,
        location=location
    )
    
    if not job:
        return None
    
    # Step 3: Wait for completion with slick progress
    final_status = wait_for_batch_completion_slick(job.name, location)
    
    if final_status and final_status['state'] == 'JOB_STATE_SUCCEEDED':
        print("ðŸ“Š Reading results...")
        results = read_batch_prediction_results_fixed(output_gcs_uri)
        return results
    else:
        print("âŒ Batch job failed or timed out")
        return None


def convert_batch_results_to_dataset(results):
    """Convert batch prediction results to structured dataset - Updated for Standardised Structure"""
    try:
        # Extract responses from batch results
        responses = []
        for i, result in enumerate(results):
            try:
                # Navigate to the actual Gemini response text
                if 'response' in result and 'candidates' in result['response']:
                    candidates = result['response']['candidates']
                    if candidates and len(candidates) > 0:
                        candidate = candidates[0]
                        if 'content' in candidate and 'parts' in candidate['content']:
                            parts = candidate['content']['parts']
                            if parts and len(parts) > 0:
                                response_text = parts[0]['text']
                                responses.append({
                                    'raw_response': response_text,
                                    'status': 'success',
                                    'response_id': i
                                })
                            else:
                                print(f"No parts found in response {i}")
                        else:
                            print(f"No content/parts in response {i}")
                    else:
                        print(f"No candidates in response {i}")
                else:
                    print(f"No response/candidates in result {i}")
                    
            except Exception as e:
                print(f"Error processing result {i}: {e}")
                responses.append({
                    'raw_response': str(result),
                    'status': 'error',
                    'response_id': i
                })
        
        print(f"Extracted {len(responses)} responses")
        
        # Parse JSON responses and flatten into separate datasets
        dashboard_data = []
        metrics_data = []
        metric_interactions_data = []
        dataset_analysis_data = []
        hardcoded_issues_data = []
        
        for response in responses:
            try:
                response_text = response['raw_response']
                
                # Extract JSON from markdown code blocks
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif '{' in response_text and '}' in response_text:
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    json_text = response_text[json_start:json_end]
                else:
                    print(f"No JSON found in response {response['response_id']}")
                    continue
                
                # Parse JSON
                parsed_response = json.loads(json_text)
                
                # Extract dashboard summary
                if 'dashboard_summary' in parsed_response:
                    dashboard_summary = parsed_response['dashboard_summary'].copy()
                    dashboard_summary['response_id'] = response['response_id']
                    
                    # Process primary_data_sources (split semicolon-separated values)
                    if 'primary_data_sources' in dashboard_summary:
                        if isinstance(dashboard_summary['primary_data_sources'], str):
                            dashboard_summary['data_sources_list'] = dashboard_summary['primary_data_sources'].split(';')
                            dashboard_summary['data_sources_count'] = len(dashboard_summary['data_sources_list'])
                        else:
                            dashboard_summary['data_sources_list'] = dashboard_summary['primary_data_sources']
                            dashboard_summary['data_sources_count'] = len(dashboard_summary['primary_data_sources']) if dashboard_summary['primary_data_sources'] else 0
                    
                    # Process hardcoded dates and values (convert to counts for analysis)
                    if 'hardcoded_dates_found' in dashboard_summary:
                        dashboard_summary['hardcoded_dates_count'] = len(dashboard_summary['hardcoded_dates_found']) if dashboard_summary['hardcoded_dates_found'] else 0
                    if 'hardcoded_values_found' in dashboard_summary:
                        dashboard_summary['hardcoded_values_count'] = len(dashboard_summary['hardcoded_values_found']) if dashboard_summary['hardcoded_values_found'] else 0
                    if 'governance_opportunities' in dashboard_summary:
                        dashboard_summary['governance_opportunities_count'] = len(dashboard_summary['governance_opportunities']) if dashboard_summary['governance_opportunities'] else 0
                    
                    dashboard_data.append(dashboard_summary)
                
                # Extract dataset analysis
                if 'dataset_analysis' in parsed_response:
                    dataset_analysis = parsed_response['dataset_analysis'].copy()
                    dataset_analysis['response_id'] = response['response_id']
                    dataset_analysis['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                    
                    # Extract hardcoded issues for separate analysis
                    if 'hardcoded_issues' in dataset_analysis:
                        hardcoded_issues = dataset_analysis['hardcoded_issues']
                        
                        # Process hardcoded dates
                        if 'hardcoded_dates' in hardcoded_issues:
                            for date_issue in hardcoded_issues['hardcoded_dates']:
                                date_issue_row = date_issue.copy()
                                date_issue_row['response_id'] = response['response_id']
                                date_issue_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                                date_issue_row['issue_type'] = 'hardcoded_date'
                                hardcoded_issues_data.append(date_issue_row)
                        
                        # Process hardcoded variables
                        if 'hardcoded_variables' in hardcoded_issues:
                            for var_issue in hardcoded_issues['hardcoded_variables']:
                                var_issue_row = var_issue.copy()
                                var_issue_row['response_id'] = response['response_id']
                                var_issue_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                                var_issue_row['issue_type'] = 'hardcoded_variable'
                                # Process hardcoded_values list
                                if 'hardcoded_values' in var_issue_row:
                                    var_issue_row['hardcoded_values_count'] = len(var_issue_row['hardcoded_values']) if var_issue_row['hardcoded_values'] else 0
                                    var_issue_row['hardcoded_values_text'] = ', '.join(var_issue_row['hardcoded_values']) if var_issue_row['hardcoded_values'] else ''
                                hardcoded_issues_data.append(var_issue_row)
                    
                    dataset_analysis_data.append(dataset_analysis)
                
                # Extract metrics
                if 'metrics' in parsed_response:
                    for metric in parsed_response['metrics']:
                        metric_row = metric.copy()
                        metric_row['response_id'] = response['response_id']
                        metric_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                        metric_row['dashboard_name'] = parsed_response.get('dashboard_summary', {}).get('dashboard_name', '')
                        
                        # Process list fields for easier analysis
                        if 'depends_on_metrics' in metric_row:
                            metric_row['depends_on_metrics_count'] = len(metric_row['depends_on_metrics']) if metric_row['depends_on_metrics'] else 0
                            metric_row['depends_on_metrics_text'] = ', '.join(metric_row['depends_on_metrics']) if metric_row['depends_on_metrics'] else ''
                        
                        if 'data_sources' in metric_row:
                            metric_row['data_sources_count'] = len(metric_row['data_sources']) if metric_row['data_sources'] else 0
                            metric_row['data_sources_text'] = ', '.join(metric_row['data_sources']) if metric_row['data_sources'] else ''
                        
                        if 'filters_applied' in metric_row:
                            metric_row['filters_applied_count'] = len(metric_row['filters_applied']) if metric_row['filters_applied'] else 0
                            metric_row['filters_applied_text'] = ', '.join(metric_row['filters_applied']) if metric_row['filters_applied'] else ''
                        
                        if 'hardcoded_dates_in_metric' in metric_row:
                            metric_row['hardcoded_dates_in_metric_count'] = len(metric_row['hardcoded_dates_in_metric']) if metric_row['hardcoded_dates_in_metric'] else 0
                        
                        if 'hardcoded_values_in_metric' in metric_row:
                            metric_row['hardcoded_values_in_metric_count'] = len(metric_row['hardcoded_values_in_metric']) if metric_row['hardcoded_values_in_metric'] else 0
                        
                        if 'governance_issues' in metric_row:
                            metric_row['governance_issues_count'] = len(metric_row['governance_issues']) if metric_row['governance_issues'] else 0
                            metric_row['governance_issues_text'] = ', '.join(metric_row['governance_issues']) if metric_row['governance_issues'] else ''
                        
                        if 'data_quality_concerns' in metric_row:
                            metric_row['data_quality_concerns_count'] = len(metric_row['data_quality_concerns']) if metric_row['data_quality_concerns'] else 0
                            metric_row['data_quality_concerns_text'] = ', '.join(metric_row['data_quality_concerns']) if metric_row['data_quality_concerns'] else ''
                        
                        metrics_data.append(metric_row)
                
                # Extract metric interactions
                if 'metric_interactions' in parsed_response:
                    for interaction in parsed_response['metric_interactions']:
                        interaction_row = interaction.copy()
                        interaction_row['response_id'] = response['response_id']
                        interaction_row['dashboard_id'] = parsed_response.get('dashboard_summary', {}).get('dashboard_id', '')
                        
                        # Process related_metrics list
                        if 'related_metrics' in interaction_row:
                            interaction_row['related_metrics_count'] = len(interaction_row['related_metrics']) if interaction_row['related_metrics'] else 0
                            interaction_row['related_metrics_text'] = ', '.join(interaction_row['related_metrics']) if interaction_row['related_metrics'] else ''
                        
                        metric_interactions_data.append(interaction_row)
                
            except json.JSONDecodeError as e:
                print(f"JSON parsing error for response {response['response_id']}: {e}")
                print(f"Response preview: {response['raw_response'][:200]}...")
            except Exception as e:
                print(f"Error processing response {response['response_id']}: {e}")
        
        # Create DataFrames
        dashboards_df = pd.DataFrame(dashboard_data) if dashboard_data else pd.DataFrame()
        metrics_df = pd.DataFrame(metrics_data) if metrics_data else pd.DataFrame()
        metric_interactions_df = pd.DataFrame(metric_interactions_data) if metric_interactions_data else pd.DataFrame()
        dataset_analysis_df = pd.DataFrame(dataset_analysis_data) if dataset_analysis_data else pd.DataFrame()
        hardcoded_issues_df = pd.DataFrame(hardcoded_issues_data) if hardcoded_issues_data else pd.DataFrame()
        
        print(f"âœ“ Created datasets:")
        print(f"  - Dashboards: {len(dashboards_df)} rows")
        print(f"  - Metrics: {len(metrics_df)} rows")
        print(f"  - Metric Interactions: {len(metric_interactions_df)} rows")
        print(f"  - Dataset Analysis: {len(dataset_analysis_df)} rows")
        print(f"  - Hardcoded Issues: {len(hardcoded_issues_df)} rows")
        
        return {
            'dashboards': dashboards_df,
            'metrics': metrics_df,
            'metric_interactions': metric_interactions_df,
            'dataset_analysis': dataset_analysis_df,
            'hardcoded_issues': hardcoded_issues_df,
            'raw_responses': pd.DataFrame(responses)
        }
        
    except Exception as e:
        print(f"âœ— Failed to convert results: {e}")
        return None


# Legacy function for compatibility
def run_gemini_batch_fast(requests, project, display_name, input_gcs_uri,
                         output_gcs_uri, model_name="gemini-2.5-flash",
                         location="us-central1", max_output_tokens=65000,
                         temperature=0.20):
    """Legacy function - redirects to slick version"""
    return run_gemini_batch_fast_slick(requests, project, display_name, input_gcs_uri,
                                      output_gcs_uri, model_name, location, max_output_tokens, temperature)

def prepare_looker_analysis_batch(df):
    """Convert dataframe to batch input format with structured Looker analysis prompt."""
    batch_data = []
    
    for dashboard_id, group in df.groupby('looker_studio_report_id'):
        dashboard_data = {
            "dashboard_id": dashboard_id,
            "dashboard_name": group.iloc[0]['looker_studio_report_name'],
            "dashboard_owner": group.iloc[0]['assetOwner'],
            "sql_samples": []
        }
        
        for _, row in group.iterrows():
            dashboard_data["sql_samples"].append({
                "job_id": row['jobId'],
                "username": row['username'],
                "runtime_seconds": row['runtime_seconds'],
                "total_processed_bytes": row['totalProcessedBytes'] if pd.notna(row['totalProcessedBytes']) else None,  # Handle NaN
                "sql_query": row['query_text']
            })
        
        # Format the structured prompt for this dashboard
        formatted_prompt = LOOKER_ANALYSIS_PROMPT.format(
            dashboard_id=dashboard_data["dashboard_id"],
            dashboard_name=dashboard_data["dashboard_name"],
            sql_samples=json.dumps(dashboard_data["sql_samples"], indent=2, default=str)  # Add default=str
        )
        
        batch_data.append({"content": formatted_prompt})
    
    return batch_data


def save_datasets_to_csv(datasets, output_folder="./data/"):
    """Save datasets to CSV files"""
    try:
        import os
        os.makedirs(output_folder, exist_ok=True)

        # Save each dataset
        for name, df in datasets.items():
            if df is not None and len(df) > 0:
                output_path = f"{output_folder}looker_analysis_{name}.csv"
                df.to_csv(output_path, index=False)
                print(f"âœ“ Saved {name}: {output_path} ({len(df)} rows)")
            else:
                print(f"âš ï¸ Skipped {name}: empty dataset")
        
        return True 
        
    except Exception as e:
        print(f"âœ— Failed to save datasets: {e}")
        return False

def analyse_results_summary(datasets):
    """Quick analysis of the results"""
    print("\n" + "="*60)
    print("LOOKER ANALYSIS RESULTS SUMMARY")
    print("="*60)
    
    if 'dashboards' in datasets and len(datasets['dashboards']) > 0:
        dashboards_df = datasets['dashboards']
        print(f"\nðŸ“Š DASHBOARDS ANALYSED: {len(dashboards_df)}")
        
        if 'business_domain' in dashboards_df.columns:
            print("\nDomains:")
            print(dashboards_df['business_domain'].value_counts())
        
        if 'complexity_score' in dashboards_df.columns:
            print(f"\nComplexity scores:")
            print(f"  Average: {dashboards_df['complexity_score'].mean():.1f}")
            print(f"  Range: {dashboards_df['complexity_score'].min()} - {dashboards_df['complexity_score'].max()}")
    
    if 'metrics' in datasets and len(datasets['metrics']) > 0:
        metrics_df = datasets['metrics']
        print(f"\nðŸ“ˆ METRICS IDENTIFIED: {len(metrics_df)}")
        
        if 'metric_type' in metrics_df.columns:
            print("\nMetric types:")
            print(metrics_df['metric_type'].value_counts())
        
        if 'calculation_type' in metrics_df.columns:
            print("\nCalculation types:")
            print(metrics_df['calculation_type'].value_counts())
        
        if 'is_final_output' in metrics_df.columns:
            final_outputs = len(metrics_df[metrics_df['is_final_output'] == True])
            print(f"\nFinal output metrics: {final_outputs}")
    
    if 'raw_responses' in datasets and len(datasets['raw_responses']) > 0:
        raw_df = datasets['raw_responses']
        successful = len(raw_df[raw_df['status'] == 'success'])
        failed = len(raw_df[raw_df['status'] == 'error'])
        print(f"\nðŸ“ˆ PROCESSING SUMMARY:")
        print(f"  Successful responses: {successful}")
        print(f"  Failed responses: {failed}")
    
    print("\n" + "="*60)


def execute_sql_with_metadata(sql, query_type, dashboard_id, response_id, client=None, project=None):
    """Execute a single SQL query and return results with metadata"""
    import time
    from datetime import datetime
    
    if client is None:
        from google.cloud import bigquery
        client = bigquery.Client(project=project)
    
    start_time = time.time()
    
    try:
        # Execute the query
        job = client.query(sql)
        results = job.result()
        
        # Convert to DataFrame
        df = results.to_dataframe()
        
        execution_time = time.time() - start_time
        
        # Create metadata
        metadata = {
            'dashboard_id': dashboard_id,
            'response_id': response_id,
            'query_type': query_type,
            'execution_status': 'success',
            'execution_time_seconds': round(execution_time, 2),
            'row_count': len(df),
            'column_count': len(df.columns),
            'columns': list(df.columns),
            'executed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'bytes_processed': job.total_bytes_processed if hasattr(job, 'total_bytes_processed') else None,
            'slot_ms': job.slot_millis if hasattr(job, 'slot_millis') else None,
            'error_message': None
        }
        
        print(f"âœ“ {query_type} for {dashboard_id}: {len(df)} rows, {execution_time:.2f}s")
        
        return {
            'data': df,
            'metadata': metadata,
            'sql': sql
        }
        
    except Exception as e:
        execution_time = time.time() - start_time
        
        metadata = {
            'dashboard_id': dashboard_id,
            'response_id': response_id,
            'query_type': query_type,
            'execution_status': 'failed',
            'execution_time_seconds': round(execution_time, 2),
            'row_count': 0,
            'column_count': 0,
            'columns': [],
            'executed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'bytes_processed': None,
            'slot_ms': None,
            'error_message': str(e)
        }
        
        print(f"âœ— {query_type} for {dashboard_id}: FAILED - {str(e)[:100]}...")
        
        return {
            'data': None,
            'metadata': metadata,
            'sql': sql
        }

def execute_dataset_analysis_queries(dataset_df, client=None, project=None, max_queries=None):
    """Execute all SQL queries from the dataset analysis DataFrame"""
    import pandas as pd
    
    if client is None:
        from google.cloud import bigquery
        client = bigquery.Client(project=project)
    
    # Query columns to process
    query_columns = ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']
    
    all_results = []
    all_metadata = []
    
    # Limit queries if specified
    rows_to_process = dataset_df.head(max_queries) if max_queries else dataset_df
    
    print(f"Executing queries for {len(rows_to_process)} dashboards...")
    
    for idx, row in rows_to_process.iterrows():
        dashboard_id = row['dashboard_id']
        response_id = row['response_id']
        
        print(f"\n--- Processing Dashboard {idx + 1}/{len(rows_to_process)}: {dashboard_id[:8]}... ---")
        
        for query_type in query_columns:
            sql = row[query_type]
            
            if pd.isna(sql) or sql.strip() == '':
                print(f"âš ï¸ Skipping {query_type}: empty SQL")
                continue
            
            # Execute the query
            result = execute_sql_with_metadata(
                sql=sql,
                query_type=query_type,
                dashboard_id=dashboard_id,
                response_id=response_id,
                client=client,
                project=project
            )
            
            # Store results
            if result['data'] is not None:
                # Add linking columns to the data
                result['data']['dashboard_id'] = dashboard_id
                result['data']['response_id'] = response_id
                result['data']['query_type'] = query_type
                
                all_results.append({
                    'dashboard_id': dashboard_id,
                    'response_id': response_id,
                    'query_type': query_type,
                    'data': result['data']
                })
            
            all_metadata.append(result['metadata'])
    
    # Create metadata DataFrame
    metadata_df = pd.DataFrame(all_metadata)
    
    print(f"\n=== EXECUTION SUMMARY ===")
    print(f"Total queries attempted: {len(all_metadata)}")
    print(f"Successful queries: {len([m for m in all_metadata if m['execution_status'] == 'success'])}")
    print(f"Failed queries: {len([m for m in all_metadata if m['execution_status'] == 'failed'])}")
    
    if len(all_metadata) > 0:
        avg_time = sum([m['execution_time_seconds'] for m in all_metadata]) / len(all_metadata)
        total_rows = sum([m['row_count'] for m in all_metadata])
        print(f"Average execution time: {avg_time:.2f}s")
        print(f"Total rows returned: {total_rows}")
    
    return {
        'results': all_results,
        'metadata': metadata_df,
        'summary': {
            'total_queries': len(all_metadata),
            'successful_queries': len([m for m in all_metadata if m['execution_status'] == 'success']),
            'failed_queries': len([m for m in all_metadata if m['execution_status'] == 'failed']),
            'total_rows_returned': sum([m['row_count'] for m in all_metadata])
        }
    }

def combine_query_results_by_type(execution_results):
    """Combine all results by query type for easier analysis"""
    import pandas as pd
    
    results_by_type = {}
    
    for result in execution_results['results']:
        query_type = result['query_type']
        
        if query_type not in results_by_type:
            results_by_type[query_type] = []
        
        results_by_type[query_type].append(result['data'])
    
    # Combine DataFrames for each query type
    combined_results = {}
    for query_type, dfs in results_by_type.items():
        if dfs:
            try:
                combined_df = pd.concat(dfs, ignore_index=True)
                combined_results[query_type] = combined_df
                print(f"âœ“ Combined {query_type}: {len(combined_df)} total rows from {len(dfs)} dashboards")
            except Exception as e:
                print(f"âœ— Failed to combine {query_type}: {e}")
                combined_results[query_type] = None
    
    return combined_results

def analyze_query_performance(metadata_df):
    """Analyze the performance of executed queries"""
    import pandas as pd
    
    if metadata_df.empty:
        print("No metadata to analyze")
        return None
    
    print("\n=== QUERY PERFORMANCE ANALYSIS ===")
    
    # Success rates by query type
    success_by_type = metadata_df.groupby('query_type')['execution_status'].apply(
        lambda x: (x == 'success').sum() / len(x) * 100
    ).round(2)
    
    print("\nSuccess rates by query type:")
    for query_type, success_rate in success_by_type.items():
        print(f"  {query_type}: {success_rate}%")
    
    # Execution times by query type (successful only)
    successful_queries = metadata_df[metadata_df['execution_status'] == 'success']
    
    if not successful_queries.empty:
        print("\nExecution times (successful queries only):")
        time_stats = successful_queries.groupby('query_type')['execution_time_seconds'].agg(['mean', 'min', 'max']).round(2)
        for query_type, stats in time_stats.iterrows():
            print(f"  {query_type}: avg={stats['mean']}s, min={stats['min']}s, max={stats['max']}s")
        
        print("\nRow counts by query type:")
        row_stats = successful_queries.groupby('query_type')['row_count'].agg(['mean', 'min', 'max']).round(0)
        for query_type, stats in row_stats.iterrows():
            print(f"  {query_type}: avg={stats['mean']} rows, min={stats['min']}, max={stats['max']}")
    
    # Failed queries details
    failed_queries = metadata_df[metadata_df['execution_status'] == 'failed']
    if not failed_queries.empty:
        print(f"\n=== FAILED QUERIES ({len(failed_queries)}) ===")
        for _, row in failed_queries.iterrows():
            print(f"  {row['query_type']} [{row['dashboard_id'][:8]}...]: {row['error_message'][:100]}...")
    
    return {
        'success_rates': success_by_type,
        'performance_stats': time_stats if not successful_queries.empty else None,
        'failed_queries': failed_queries
    }

# Example usage function
def run_complete_analysis(dataset_df, project=None, max_dashboards=5):
    """Run complete analysis with all queries"""
    from google.cloud import bigquery
    
    # Create BigQuery client
    client = bigquery.Client(project=project) if project else bigquery.Client()
    
    print(f"Starting complete analysis for {min(max_dashboards or len(dataset_df), len(dataset_df))} dashboards...")
    
    # Execute all queries
    execution_results = execute_dataset_analysis_queries(
        dataset_df, 
        client=client, 
        project=project, 
        max_queries=max_dashboards
    )
    
    # Analyze performance
    performance_analysis = analyze_query_performance(execution_results['metadata'])
    
    # Combine results by type
    combined_results = combine_query_results_by_type(execution_results)
    
    return {
        'execution_results': execution_results,
        'combined_results': combined_results,
        'performance_analysis': performance_analysis
    }



def troubleshoot_failed_queries(execution_results):
    """Analyze and categorize failed queries to improve the prompt"""
    
    failed_queries = execution_results['metadata'][execution_results['metadata']['execution_status'] == 'failed']
    
    if failed_queries.empty:
        print("No failed queries to troubleshoot!")
        return
    
    print("=== FAILED QUERY ANALYSIS ===\n")
    
    error_categories = {
        'type_mismatch': [],
        'function_signature': [],
        'group_by_issues': [],
        'array_issues': [],
        'other': []
    }
    
    for _, row in failed_queries.iterrows():
        error_msg = row['error_message'].lower()
        
        if 'no matching signature for operator' in error_msg and ('int64' in error_msg or 'string' in error_msg):
            error_categories['type_mismatch'].append(row)
        elif 'no matching signature for function' in error_msg:
            error_categories['function_signature'].append(row)
        elif 'neither grouped nor' in error_msg or 'group by' in error_msg:
            error_categories['group_by_issues'].append(row)
        elif 'array cannot have a null element' in error_msg:
            error_categories['array_issues'].append(row)
        else:
            error_categories['other'].append(row)
    
    # Print categorized errors
    for category, errors in error_categories.items():
        if errors:
            print(f"\n{category.upper().replace('_', ' ')} ({len(errors)} errors):")
            for error in errors:
                print(f"  Dashboard: {error['dashboard_id'][:8]}...")
                print(f"  Query Type: {error['query_type']}")
                print(f"  Error: {error['error_message'][:150]}...")
                print()
    
    return error_categories

def get_failed_sql_for_inspection(dataset_df, execution_results, error_type='type_mismatch'):
    """Get the actual SQL from failed queries for inspection"""
    
    failed_metadata = execution_results['metadata'][execution_results['metadata']['execution_status'] == 'failed']
    
    print(f"=== FAILED SQL INSPECTION ({error_type.upper()}) ===\n")
    
    for _, row in failed_metadata.head(2).iterrows():  # Show first 2 failed queries
        dashboard_id = row['dashboard_id']
        query_type = row['query_type']
        
        # Get the SQL from original dataset
        original_row = dataset_df[dataset_df['dashboard_id'] == dashboard_id].iloc[0]
        sql = original_row[query_type]
        
        print(f"Dashboard: {dashboard_id}")
        print(f"Query Type: {query_type}")
        print(f"Error: {row['error_message']}")
        print("\nSQL:")
        print("-" * 80)
        print(sql[:500] + "..." if len(sql) > 500 else sql)
        print("-" * 80)
        print("\n")

def auto_git_push(commit_message=None, files_to_add=".", include_timestamp=True, 
                  dry_run=False, force_push=False):
    """
    Automatically commit and push changes to git
    
    Args:
        commit_message (str): Custom commit message
        files_to_add (str): Files to add ('.' for all, or specific files)
        include_timestamp (bool): Add timestamp to commit message
        dry_run (bool): Show what would be done without doing it
        force_push (bool): Force push even if no changes detected
    """
    import subprocess
    import os
    from datetime import datetime
    
    def run_git_command(command, capture_output=True):
        """Run git command and return result"""
        try:
            if dry_run:
                print(f"[DRY RUN] Would run: {command}")
                return True, ""
            
            result = subprocess.run(command, shell=True, capture_output=capture_output, 
                                  text=True, cwd='/content/looker-metrics')
            return result.returncode == 0, result.stdout + result.stderr
        except Exception as e:
            return False, str(e)
    
    print("ðŸ”„ Auto Git Push Starting...")
    
    # Check if we're in a git repository
    success, output = run_git_command("git status --porcelain")
    if not success:
        print("âŒ Not in a git repository or git error occurred")
        return False
    
    # Check if there are changes
    if not output.strip() and not force_push:
        print("âœ… No changes to commit")
        return True
    
    print(f"ðŸ“ Changes detected:")
    if not dry_run:
        run_git_command("git status --short", capture_output=False)
    
    # Add files
    print(f"ðŸ“¤ Adding files: {files_to_add}")
    success, output = run_git_command(f"git add {files_to_add}")
    if not success:
        print(f"âŒ Failed to add files: {output}")
        return False
    
    # Create commit message
    if commit_message is None:
        commit_message = "Auto-commit: Updated looker analysis code"
    
    if include_timestamp:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        commit_message = f"{commit_message} - {timestamp}"
    
    # Commit changes
    print(f"ðŸ’¾ Committing: {commit_message}")
    success, output = run_git_command(f'git commit -m "{commit_message}"')
    if not success:
        if "nothing to commit" in output:
            print("âœ… Nothing new to commit")
            return True
        else:
            print(f"âŒ Failed to commit: {output}")
            return False
    
    # Push to remote
    print("ðŸš€ Pushing to GitHub...")
    success, output = run_git_command("git push origin main")
    if not success:
        print(f"âŒ Failed to push: {output}")
        return False
    
    print("âœ… Successfully pushed to GitHub!")
    return True

def setup_auto_git_config(username, email, token):
    """
    One-time setup for git configuration and authentication
    
    Args:
        username (str): Your GitHub username
        email (str): Your email address  
        token (str): Your GitHub Personal Access Token
    """
    import subprocess
    
    def run_command(command):
        try:
            result = subprocess.run(command, shell=True, capture_output=True, 
                                  text=True, cwd='/content/looker-metrics')
            return result.returncode == 0, result.stdout + result.stderr
        except Exception as e:
            return False, str(e)
    
    print("âš™ï¸ Setting up git configuration...")
    
    # Set user identity
    run_command(f'git config --global user.email "{email}"')
    run_command(f'git config --global user.name "{username}"')
    
    # Set remote URL with authentication
    repo_url = f"https://{username}:{token}@github.com/richie-peters/looker-metrics.git"
    success, output = run_command(f'git remote set-url origin {repo_url}')
    
    if success:
        print("âœ… Git configuration complete!")
        return True
    else:
        print(f"âŒ Git configuration failed: {output}")
        return False

def create_auto_backup_decorator():
    """
    Create a decorator that automatically backs up after running functions
    """
    def auto_backup_decorator(func):
        def wrapper(*args, **kwargs):
            # Run the original function
            result = func(*args, **kwargs)
            
            # Auto-backup after function completes
            print(f"\nðŸ”„ Auto-backup after {func.__name__}...")
            auto_git_push(
                commit_message=f"Auto-backup after running {func.__name__}",
                include_timestamp=True
            )
            
            return result
        return wrapper
    return auto_backup_decorator

# Usage examples and setup
def setup_git_auto_push():
    """Setup function - run this once"""
    print("Setting up auto-git-push...")
    
    # You'll need to provide these
    USERNAME = "richie-peters"
    EMAIL = "richie.peters@news.com.au"  
    TOKEN = "your_github_token_here"  # Replace with your actual token
    
    # Setup git config
    setup_auto_git_config(USERNAME, EMAIL, TOKEN)
    
    print("\nðŸŽ¯ Auto-push functions ready to use!")
    print("\nUsage examples:")
    print("  auto_git_push()  # Simple push with default message")
    print("  auto_git_push('Fixed bug in analysis')  # Custom message")
    print("  auto_git_push(dry_run=True)  # See what would happen")

# Convenience functions
def quick_push(message="Quick update"):
    """Quick push with simple message"""
    return auto_git_push(commit_message=message)

def save_progress(description="Progress checkpoint"):
    """Save current progress"""
    return auto_git_push(commit_message=f"Progress: {description}")

def backup_now():
    """Emergency backup"""
    return auto_git_push(commit_message="Emergency backup", force_push=True)

# Example: Auto-backup decorator usage
@create_auto_backup_decorator()
def my_analysis_function():
    """Example function that auto-backs up when it finishes"""
    print("Doing some analysis...")
    # Your analysis code here
    return "Analysis complete"

print("âœ… Auto-git-push functions loaded!")
print("\nðŸš€ To get started:")
print("1. Run: setup_git_auto_push()  # One-time setup")
print("2. Then use: auto_git_push() or quick_push('your message')")

def fix_github_authentication():
    """Fix GitHub authentication with Personal Access Token"""
    
    # You need to paste your actual token here
    USERNAME = "richie-peters"
    TOKEN = input("Paste your GitHub Personal Access Token here: ")  # This will prompt you to enter it
    
    if not TOKEN or TOKEN.strip() == "":
        print("âŒ No token provided!")
        return False
    
    # Remove any whitespace
    TOKEN = TOKEN.strip()
    
    # Set the remote URL with token authentication
    import subprocess
    
    repo_url = f"https://{USERNAME}:{TOKEN}@github.com/richie-peters/looker-metrics.git"
    
    try:
        # Update the remote URL
        result = subprocess.run(
            f'git remote set-url origin {repo_url}', 
            shell=True, 
            capture_output=True, 
            text=True, 
            cwd='/content/looker-metrics'
        )
        
        if result.returncode == 0:
            print("âœ… GitHub authentication updated successfully!")
            
            # Test the connection
            test_result = subprocess.run(
                'git remote -v', 
                shell=True, 
                capture_output=True, 
                text=True, 
                cwd='/content/looker-metrics'
            )
            
            if "https://" in test_result.stdout:
                print("âœ… Remote URL configured correctly")
                return True
            else:
                print("âš ï¸ Remote URL might not be set correctly")
                return False
        else:
            print(f"âŒ Failed to set remote URL: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Error setting up authentication: {e}")
        return False


def check_git_status():
    """Check detailed git status"""
    import subprocess
    import os
    
    os.chdir('/content/looker-metrics')
    
    print("=== CURRENT GIT STATUS ===")
    
    # Check working directory status
    result = subprocess.run('git status', shell=True, capture_output=True, text=True)
    print("Working directory status:")
    print(result.stdout)
    
    # Check if there are unpushed commits
    print("\n=== UNPUSHED COMMITS ===")
    result = subprocess.run('git log origin/main..HEAD --oneline', shell=True, capture_output=True, text=True)
    if result.stdout.strip():
        print("Unpushed commits:")
        print(result.stdout)
    else:
        print("No unpushed commits")
    
    # Check recent commits
    print("\n=== RECENT COMMITS ===")
    result = subprocess.run('git log --oneline -5', shell=True, capture_output=True, text=True)
    print(result.stdout)


def push_existing_commits():
    """Push any existing unpushed commits"""
    import subprocess
    import os
    
    os.chdir('/content/looker-metrics')
    
    # Check for unpushed commits
    result = subprocess.run('git log origin/main..HEAD --oneline', shell=True, capture_output=True, text=True)
    
    if result.stdout.strip():
        print("ðŸ“¤ Found unpushed commits, pushing now...")
        push_result = subprocess.run('git push origin main', shell=True, capture_output=True, text=True)
        
        if push_result.returncode == 0:
            print("âœ… Successfully pushed existing commits!")
            return True
        else:
            print(f"âŒ Push failed: {push_result.stderr}")
            return False
    else:
        print("âœ… No unpushed commits found")
        return True

def create_unified_dataset_from_existing(datasets, df_bq_results=None):
    """
    Create unified dataset from your existing separate datasets
    """
    import pandas as pd
    
    print("ðŸ”— CREATING UNIFIED DATASET FROM YOUR EXISTING DATA")
    print("=" * 55)
    
    # Get your datasets
    dashboards_df = datasets.get('dashboards', pd.DataFrame())
    metrics_df = datasets.get('metrics', pd.DataFrame()) 
    metric_interactions_df = datasets.get('metric_interactions', pd.DataFrame())
    dataset_analysis_df = datasets.get('dataset_analysis', pd.DataFrame())
    hardcoded_issues_df = datasets.get('hardcoded_issues', pd.DataFrame())
    
    print(f"Input datasets:")
    print(f"  Dashboards: {len(dashboards_df)} rows")
    print(f"  Metrics: {len(metrics_df)} rows") 
    print(f"  Metric Interactions: {len(metric_interactions_df)} rows")
    print(f"  Dataset Analysis: {len(dataset_analysis_df)} rows")
    print(f"  Hardcoded Issues: {len(hardcoded_issues_df)} rows")
    
    unified_records = []
    
    # Create dashboard-level records
    print("\nðŸ“Š Creating dashboard-level records...")
    for _, dashboard in dashboards_df.iterrows():
        dashboard_id = dashboard['dashboard_id']
        
        # Get related data for this dashboard
        dashboard_metrics = metrics_df[metrics_df['dashboard_id'] == dashboard_id] if not metrics_df.empty else pd.DataFrame()
        dashboard_interactions = metric_interactions_df[metric_interactions_df['dashboard_id'] == dashboard_id] if not metric_interactions_df.empty else pd.DataFrame()
        dashboard_analysis = dataset_analysis_df[dataset_analysis_df['dashboard_id'] == dashboard_id] if not dataset_analysis_df.empty else pd.DataFrame()
        dashboard_issues = hardcoded_issues_df[hardcoded_issues_df['dashboard_id'] == dashboard_id] if not hardcoded_issues_df.empty else pd.DataFrame()
        
        # Create dashboard summary record  
        dashboard_record = {
            # Core identifiers
            'record_id': f"{dashboard_id}_dashboard",
            'dashboard_id': dashboard_id,
            'response_id': dashboard.get('response_id', ''),
            'record_type': 'dashboard_summary',
            
            # Dashboard info
            'dashboard_name': dashboard.get('dashboard_name', ''),
            'business_domain': dashboard.get('business_domain', ''),
            'complexity_score': dashboard.get('complexity_score', 0),
            'consolidation_score': dashboard.get('consolidation_score', 0),
            'date_grain': dashboard.get('date_grain', ''),
            'data_grain': dashboard.get('data_grain', ''),
            'primary_data_sources': dashboard.get('primary_data_sources', ''),
            'date_range_detected': dashboard.get('date_range_detected', ''),
            
            # Aggregated metrics info
            'total_metrics_count': len(dashboard_metrics),
            'kpi_metrics_count': len(dashboard_metrics[dashboard_metrics.get('is_kpi', False) == True]) if not dashboard_metrics.empty else 0,
            'final_output_metrics_count': len(dashboard_metrics[dashboard_metrics.get('is_final_output', False) == True]) if not dashboard_metrics.empty else 0,
            
            # Governance summary
            'hardcoded_dates_count': dashboard.get('hardcoded_dates_count', 0),
            'hardcoded_values_count': dashboard.get('hardcoded_values_count', 0),
            'governance_issues_count': len(dashboard_issues),
            
            # Interactions
            'metric_interactions_count': len(dashboard_interactions),
            
            # Text summaries for embeddings
            'dashboard_description': create_dashboard_description(dashboard, dashboard_metrics),
            'metrics_summary': create_metrics_summary_text(dashboard_metrics),
            'governance_summary': create_governance_summary_text(dashboard_issues),
            'full_context': 'dashboard_summary'
        }
        
        unified_records.append(dashboard_record)
        
        # Create individual metric records
        for _, metric in dashboard_metrics.iterrows():
            metric_record = {
                'record_id': f"{dashboard_id}_{metric.get('metric_id', 'unknown')}",
                'dashboard_id': dashboard_id,
                'response_id': metric.get('response_id', ''),
                'record_type': 'metric',
                
                # Link to dashboard
                'dashboard_name': dashboard.get('dashboard_name', ''),
                'business_domain': dashboard.get('business_domain', ''),
                
                # Metric details
                'metric_id': metric.get('metric_id', ''),
                'metric_name': metric.get('metric_name', ''),
                'metric_type': metric.get('metric_type', ''),
                'calculation_type': metric.get('calculation_type', ''),
                'is_kpi': metric.get('is_kpi', False),
                'is_final_output': metric.get('is_final_output', False),
                'business_criticality': metric.get('business_criticality', ''),
                'metric_category': metric.get('metric_category', ''),
                'business_description': metric.get('business_description', ''),
                'sql_logic': metric.get('sql_logic', ''),
                
                # Dependencies and relationships
                'depends_on_metrics_count': metric.get('depends_on_metrics_count', 0),
                'data_sources_count': metric.get('data_sources_count', 0),
                'governance_issues_count': metric.get('governance_issues_count', 0),
                
                # Text description
                'dashboard_description': create_metric_description(metric, dashboard),
                'metrics_summary': metric.get('business_description', ''),
                'governance_summary': metric.get('governance_issues_text', ''),
                'full_context': 'individual_metric'
            }
            
            unified_records.append(metric_record)
    
    # Convert to DataFrame
    unified_df = pd.DataFrame(unified_records)
    
    print(f"\nâœ… Created unified dataset:")
    print(f"   Total records: {len(unified_df)}")
    print(f"   Dashboard summaries: {len(unified_df[unified_df['record_type'] == 'dashboard_summary'])}")
    print(f"   Individual metrics: {len(unified_df[unified_df['record_type'] == 'metric'])}")
    print(f"   Columns: {len(unified_df.columns)}")
    
    return unified_df

def create_dashboard_description(dashboard, metrics):
    """Create readable description of dashboard"""
    parts = []
    
    name = dashboard.get('dashboard_name', 'Unknown Dashboard')
    domain = dashboard.get('business_domain', 'unknown')
    
    parts.append(f"{name} is a {domain} dashboard")
    
    if not metrics.empty:
        kpis = len(metrics[metrics.get('is_kpi', False) == True])
        total = len(metrics)
        parts.append(f"containing {total} metrics")
        if kpis > 0:
            parts.append(f"including {kpis} key performance indicators")
    
    complexity = dashboard.get('complexity_score', 0)
    if complexity > 7:
        parts.append("with high analytical complexity")
    elif complexity > 4:
        parts.append("with moderate complexity")
    
    sources = dashboard.get('primary_data_sources', '')
    if sources:
        source_count = len(sources.split(';'))
        parts.append(f"drawing from {source_count} data sources")
    
    return '. '.join(parts) + '.'

def create_metrics_summary_text(metrics):
    """Create text summary of metrics"""
    if metrics.empty:
        return "No metrics defined."
    
    parts = []
    total = len(metrics)
    kpis = len(metrics[metrics.get('is_kpi', False) == True])
    
    parts.append(f"Contains {total} metrics")
    if kpis > 0:
        parts.append(f"{kpis} are key performance indicators")
    
    if 'metric_category' in metrics.columns:
        categories = metrics['metric_category'].value_counts()
        if not categories.empty:
            top_cat = categories.index[0]
            parts.append(f"primarily focused on {top_cat} metrics")
    
    return '. '.join(parts) + '.'

def create_governance_summary_text(issues):
    """Create governance issues summary"""
    if issues.empty:
        return "No governance issues identified."
    
    parts = []
    total = len(issues)
    parts.append(f"Has {total} governance issues")
    
    if 'issue_type' in issues.columns:
        issue_types = issues['issue_type'].value_counts()
        for issue_type, count in issue_types.head(2).items():
            parts.append(f"{count} {issue_type.replace('_', ' ')} issues")
    
    return '. '.join(parts) + '.'

def create_metric_description(metric, dashboard):
    """Create description for individual metric"""
    parts = []
    
    name = metric.get('metric_name', 'Unknown Metric')
    mtype = metric.get('metric_type', 'unknown')
    calc_type = metric.get('calculation_type', 'unknown')
    
    parts.append(f"{name} is a {mtype} metric using {calc_type} calculation")
    
    if metric.get('is_kpi'):
        parts.append("classified as a key performance indicator")
    
    if metric.get('business_description'):
        parts.append(f"measuring {metric['business_description']}")
    
    parts.append(f"from the {dashboard.get('dashboard_name', 'unknown')} dashboard")
    
    return '. '.join(parts) + '.'

def prepare_secondary_batch_input_clean(unified_dataset, df_bq_results):
    """
    Clean version - only use dashboard summary records
    """
    import json
    
    print("ðŸ“Š PREPARING SECONDARY BATCH INPUT (CLEAN VERSION)")
    print("=" * 55)
    
    # Filter to only dashboard summary records and remove NaN columns
    dashboard_summaries = unified_dataset[
        unified_dataset['record_type'] == 'dashboard_summary'
    ].dropna(axis=1, how='all')
    
    print(f"Processing {len(dashboard_summaries)} dashboard summaries")
    
    secondary_batch_data = []
    
    # Get the combined BQ results
    combined_results = df_bq_results.get('combined_results', {})
    
    for _, dashboard in dashboard_summaries.iterrows():
        dashboard_id = dashboard['dashboard_id']
        
        print(f"  Processing: {dashboard.get('dashboard_name', 'Unknown')[:50]}...")
        
        # Get original analysis data - only use non-NaN values
        original_analysis = {}
        for key in ['dashboard_name', 'business_domain', 'complexity_score', 'consolidation_score', 
                   'total_metrics_count', 'primary_data_sources', 'date_grain', 'data_grain',
                   'kpi_metrics_count', 'governance_issues_count']:
            value = dashboard.get(key)
            if pd.notna(value):
                original_analysis[key] = value
        
        original_analysis['dashboard_id'] = dashboard_id
        
        # Get actual SQL results for this dashboard
        actual_results = {}
        
        for query_type in ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']:
            if query_type in combined_results and combined_results[query_type] is not None:
                dashboard_data = combined_results[query_type][
                    combined_results[query_type]['dashboard_id'] == dashboard_id
                ]
                
                if not dashboard_data.empty:
                    # Convert to JSON-serializable format
                    actual_results[query_type] = {
                        'row_count': len(dashboard_data),
                        'columns': [col for col in dashboard_data.columns if col not in ['dashboard_id', 'response_id', 'query_type']],
                        'sample_data': dashboard_data.head(3).to_dict('records'),
                        'data_summary': f"Dataset contains {len(dashboard_data)} rows with {len(dashboard_data.columns)} columns"
                    }
                else:
                    actual_results[query_type] = {'row_count': 0, 'message': 'No data returned'}
            else:
                actual_results[query_type] = {'message': 'Query not executed or failed'}
        
        # Get related metrics for this dashboard from the unified dataset
        dashboard_metrics = unified_dataset[
            (unified_dataset['dashboard_id'] == dashboard_id) & 
            (unified_dataset['record_type'] == 'metric')
        ]
        
        metrics_info = []
        for _, metric in dashboard_metrics.iterrows():
            metric_dict = {}
            for key in ['metric_id', 'metric_name', 'metric_type', 'business_description', 
                       'sql_logic', 'is_kpi', 'calculation_type', 'metric_category']:
                value = metric.get(key)
                if pd.notna(value):
                    metric_dict[key] = value
            if metric_dict:  # Only add if we have some data
                metrics_info.append(metric_dict)
        
        # Create the input data structure
        secondary_input_data = {
            'dashboard_id': dashboard_id,
            'dashboard_name': dashboard.get('dashboard_name', ''),
            'original_dashboard_analysis': original_analysis,
            'dashboard_metrics': metrics_info,
            'actual_sql_results': actual_results
        }
        
        # Get the secondary prompt
        secondary_prompt = design_secondary_analysis_prompt()
        
        # Format the prompt with the data
        try:
            formatted_prompt = secondary_prompt.format(
                dashboard_id=dashboard_id,
                dashboard_name=dashboard.get('dashboard_name', ''),
                original_dashboard_analysis=json.dumps(original_analysis, indent=2),
                actual_sql_results=json.dumps(actual_results, indent=2),
                primary_analysis_data=json.dumps(actual_results.get('primary_analysis_sql', {}), indent=2),
                structure_analysis_data=json.dumps(actual_results.get('structure_sql', {}), indent=2),
                validation_results=json.dumps(actual_results.get('validation_sql', {}), indent=2),
                business_rules_data=json.dumps(actual_results.get('business_rules_sql', {}), indent=2)
            )
            
            secondary_batch_data.append({
                'content': formatted_prompt,
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard.get('dashboard_name', '')
            })
            
        except Exception as e:
            print(f"    âš ï¸ Error formatting prompt for {dashboard_id}: {e}")
            continue
    
    print(f"âœ… Successfully prepared {len(secondary_batch_data)} secondary analysis requests")
    return secondary_batch_data


def design_secondary_analysis_prompt():
    """
    Design the secondary analysis prompt that leverages actual SQL results
    for consolidation planning and migration preparation
    """
    
    secondary_prompt = """
    SECONDARY LOOKER CONSOLIDATION ANALYSIS
    =====================================
    
    You are an expert data architect analyzing Looker Studio dashboards for a finance systems consolidation project.
    You now have ACTUAL DATA RESULTS from BigQuery queries, not just SQL analysis.
    
    INPUT DATA:
    - Dashboard ID: {dashboard_id}
    - Dashboard Name: {dashboard_name}
    - Original Analysis: {original_dashboard_analysis}
    - SQL Execution Results: {actual_sql_results}
    - Primary Analysis Data: {primary_analysis_data}
    - Structure Analysis Data: {structure_analysis_data}
    - Validation Results: {validation_results}
    - Business Rules Data: {business_rules_data}
    
    CONSOLIDATION OBJECTIVES:
    1. **METRIC CONSOLIDATION**: Identify duplicate/similar metrics across dashboards
    2. **DATA SOURCE UNIFICATION**: Map to consolidated finance data model
    3. **TRANSFORMATION MAPPING**: Create specific transformation rules
    4. **TESTING STRATEGY**: Design validation approaches for migration
    5. **RELATIONSHIP MODELING**: Design new unified data relationships
    6. **MIGRATION PLANNING**: Create step-by-step migration approach
    
    OUTPUT REQUIREMENTS (JSON):
    {{
      "consolidation_analysis": {{
        "dashboard_id": "string",
        "dashboard_name": "string",
        "consolidation_priority": "high|medium|low",
        "migration_complexity": 1-10,
        "data_quality_score": 1-10,
        "consolidation_readiness": "ready|needs_prep|major_rework",
        "estimated_migration_effort_days": number,
        "business_impact_risk": "high|medium|low",
        "dependencies": ["dashboard_id1", "dashboard_id2"],
        "consolidation_opportunities_count": number
      }},
      
      "metrics_consolidation": [
        {{
          "current_metric_name": "string",
          "current_calculation": "string",
          "data_sample_analysis": "what the actual data tells us about this metric",
          "consolidation_target_metric": "proposed unified metric name",
          "consolidation_rationale": "why these should be consolidated",
          "transformation_rule": "specific transformation logic needed",
          "data_validation_rule": "how to test this transformation",
          "business_impact": "high|medium|low",
          "migration_order": number,
          "similar_metrics_across_dashboards": ["metric references from other dashboards"],
          "unified_definition": "standardised business definition",
          "sample_values_analysis": "insights from actual data values",
          "data_quality_issues": ["specific issues found in data"],
          "proposed_new_calculation": "new standardised calculation method"
        }}
      ],
      
      "data_source_mapping": [
        {{
          "current_source": "project.dataset.table",
          "current_usage": "how this source is used",
          "data_sample_insights": "what the actual data structure reveals", 
          "consolidation_target": "proposed new unified table/view",
          "mapping_complexity": 1-10,
          "transformation_type": "direct_map|calculation_required|major_restructure",
          "key_fields_mapping": {{"old_field": "new_field"}},
          "data_quality_concerns": ["issues found in actual data"],
          "migration_prerequisites": ["steps needed before migration"],
          "testing_approach": "how to validate this mapping"
        }}
      ],
      
      "relationship_model": {{
        "current_relationships": ["how data currently connects"],
        "proposed_unified_model": "description of new consolidated model",
        "key_entities": ["primary business objects"],
        "relationship_changes": ["what relationships need to change"],
        "consolidation_benefits": ["benefits of new model"],
        "implementation_challenges": ["challenges to address"],
        "data_lineage_impact": "how this affects data flow"
      }},
      
      "transformation_specifications": [
        {{
          "transformation_name": "string", 
          "source_logic": "current calculation/logic",
          "target_logic": "new unified logic",
          "transformation_type": "formula_change|aggregation_change|source_change|business_rule_change",
          "sql_transformation": "specific SQL to perform transformation",
          "validation_sql": "SQL to validate transformation worked correctly",
          "rollback_plan": "how to reverse if issues found",
          "testing_data_sample": "sample data to test with",
          "expected_result_range": "expected output values/ranges",
          "business_validation_criteria": "how business users validate correctness"
        }}
      ],
      
      "migration_plan": {{
        "migration_wave": 1-5,
        "migration_order_within_wave": number,
        "prerequisites": ["what must be done first"],
        "migration_steps": ["specific step-by-step actions"],
        "testing_phases": ["validation checkpoints"],
        "rollback_triggers": ["conditions that require rollback"],
        "business_validation_required": ["stakeholder sign-offs needed"],
        "go_live_criteria": ["requirements for production deployment"],
        "post_migration_monitoring": ["what to monitor after migration"]
      }},
      
      "english_summaries": {{
        "dashboard_plain_english": "Simple description of what this dashboard does for business users",
        "consolidation_story": "Plain English explanation of how this fits into consolidation",
        "business_impact_summary": "What happens to users during migration",
        "key_changes_summary": "Main changes users will see",
        "benefits_summary": "Benefits users will gain from consolidation",
        "migration_timeline_summary": "When changes will happen"
      }},
      
      "quality_assessment": {{
        "data_completeness_score": 1-10,
        "data_accuracy_issues": ["problems found in actual data"],
        "calculation_validation_results": ["whether calculations work correctly"],
        "business_logic_issues": ["problems with business rules"],
        "performance_concerns": ["query performance issues"],
        "scalability_issues": ["problems with data volume"],
        "recommendations": ["specific improvements needed"]
      }}
    }}
    
    ANALYSIS REQUIREMENTS USING ACTUAL DATA:
    
    1. **DATA-DRIVEN METRIC ANALYSIS**:
       - Use actual data samples to understand metric behavior
       - Identify metrics that produce similar results (potential duplicates)
       - Analyze data distributions to understand business patterns
       - Find calculation inconsistencies by comparing results
    
    2. **CONSOLIDATION OPPORTUNITY DETECTION**:
       - Compare metrics across dashboards using actual values
       - Identify business logic that can be standardised
       - Find data sources that can be unified
       - Detect redundant calculations
    
    3. **TRANSFORMATION DESIGN**:
       - Create specific transformation rules based on actual data patterns
       - Design validation queries using real data ranges
       - Identify edge cases from actual data samples
       - Plan for data quality improvements
    
    4. **MIGRATION PLANNING**:
       - Sequence migrations based on data dependencies
       - Design testing using actual data samples
       - Plan rollback strategies with real scenarios
       - Create business validation criteria
    
    5. **PLAIN ENGLISH DOCUMENTATION**:
       - Explain dashboards in business terms
       - Describe consolidation benefits clearly
       - Create user-friendly migration communications
       - Write simple testing instructions
    
    CRITICAL ANALYSIS POINTS:
    
    - Use ACTUAL DATA VALUES to inform consolidation decisions
    - Focus on BUSINESS IMPACT and user experience
    - Design TESTABLE transformation rules
    - Create ACTIONABLE migration plans
    - Prioritise based on REAL DATA COMPLEXITY and BUSINESS VALUE
    - Consider DATA QUALITY issues found in actual results
    - Plan for CHANGE MANAGEMENT and user adoption
    
    SAMPLE DATA ANALYSIS APPROACH:
    - Compare metric results across similar dashboards
    - Identify data patterns that indicate consolidation opportunities  
    - Use actual value ranges to design validation rules
    - Analyze data quality issues for migration planning
    - Design transformation logic based on real data behavior
    
    OUTPUT FOCUS:
    Create actionable consolidation guidance that can be directly used for:
    1. Building unified data models
    2. Writing transformation code
    3. Designing test cases
    4. Planning user migration
    5. Communicating changes to stakeholders
    """
    
    return secondary_prompt


def prepare_secondary_batch_input_robust(unified_dataset, df_bq_results):
    """
    Robust version - handles dates, NaNs, and missing data gracefully
    """
    import json
    import pandas as pd
    from datetime import date, datetime
    import numpy as np
    
    print("ðŸ“Š PREPARING SECONDARY BATCH INPUT (ROBUST VERSION)")
    print("=" * 55)
    
    def clean_for_json(obj):
        """Clean data for JSON serialization"""
        if pd. isna(obj) or obj is None:
            return None
        elif isinstance(obj, (date, datetime)):
            return obj.strftime('%Y-%m-%d') if hasattr(obj, 'strftime') else str(obj)
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj) if not pd.isna(obj) else None
        elif isinstance(obj, dict):
            return {k: clean_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [clean_for_json(item) for item in obj]
        else:
            return obj
    
    # Filter to dashboard summaries only
    dashboard_summaries = unified_dataset[
        unified_dataset['record_type'] == 'dashboard_summary'
    ]
    
    print(f"Processing {len(dashboard_summaries)} dashboard summaries")
    
    secondary_batch_data = []
    combined_results = df_bq_results.get('combined_results', {})
    
    for _, dashboard in dashboard_summaries.iterrows():
        dashboard_id = dashboard['dashboard_id']
        dashboard_name = dashboard.get('dashboard_name', 'Unknown Dashboard')
        
        print(f"  Processing: {dashboard_name[:50]}...")
        
        try:
            # Create original analysis with cleaned data
            original_analysis = {
                'dashboard_id': dashboard_id,
                'dashboard_name': clean_for_json(dashboard.get('dashboard_name')),
                'business_domain': clean_for_json(dashboard.get('business_domain')),
                'complexity_score': clean_for_json(dashboard.get('complexity_score')),
                'consolidation_score': clean_for_json(dashboard.get('consolidation_score')),
                'total_metrics_count': clean_for_json(dashboard.get('total_metrics_count')),
                'kpi_metrics_count': clean_for_json(dashboard.get('kpi_metrics_count')),
                'data_grain': clean_for_json(dashboard.get('data_grain')),
                'date_grain': clean_for_json(dashboard.get('date_grain')),
                'governance_issues_count': clean_for_json(dashboard.get('governance_issues_count'))
            }
            
            # Remove None values
            original_analysis = {k: v for k, v in original_analysis.items() if v is not None}
            
            # Get SQL results - proceed even if some are missing
            actual_results = {}
            
            for query_type in ['primary_analysis_sql', 'structure_sql', 'validation_sql', 'business_rules_sql']:
                try:
                    if query_type in combined_results and combined_results[query_type] is not None:
                        dashboard_data = combined_results[query_type][
                            combined_results[query_type]['dashboard_id'] == dashboard_id
                        ]
                        
                        if not dashboard_data.empty:
                            # Get a small sample and clean it
                            sample_data = dashboard_data.head(3).copy()
                            
                            # Clean sample data for JSON serialization
                            cleaned_sample = []
                            for _, row in sample_data.iterrows():
                                cleaned_row = {}
                                for col, val in row.items():
                                    if col not in ['dashboard_id', 'response_id', 'query_type']:
                                        cleaned_val = clean_for_json(val)
                                        if cleaned_val is not None:
                                            cleaned_row[col] = cleaned_val
                                if cleaned_row:  # Only add if we have some data
                                    cleaned_sample.append(cleaned_row)
                            
                            actual_results[query_type] = {
                                'status': 'success',
                                'row_count': len(dashboard_data),
                                'columns': [col for col in dashboard_data.columns 
                                          if col not in ['dashboard_id', 'response_id', 'query_type']],
                                'sample_data': cleaned_sample[:2],  # Just 2 rows to keep prompt manageable
                                'data_summary': f"Contains {len(dashboard_data)} rows with business data"
                            }
                        else:
                            actual_results[query_type] = {
                                'status': 'no_data',
                                'message': 'Query executed but returned no rows for this dashboard'
                            }
                    else:
                        actual_results[query_type] = {
                            'status': 'not_available',
                            'message': 'Query not executed or failed - will analyse based on available data'
                        }
                except Exception as e:
                    actual_results[query_type] = {
                        'status': 'error',
                        'message': f'Error processing query results: {str(e)[:100]}'
                    }
            
            # Get metrics for this dashboard
            dashboard_metrics = unified_dataset[
                (unified_dataset['dashboard_id'] == dashboard_id) & 
                (unified_dataset['record_type'] == 'metric')
            ]
            
            metrics_info = []
            for _, metric in dashboard_metrics.iterrows():
                metric_dict = {}
                for key in ['metric_id', 'metric_name', 'metric_type', 'business_description', 
                           'is_kpi', 'calculation_type', 'metric_category']:
                    value = clean_for_json(metric.get(key))
                    if value is not None:
                        metric_dict[key] = value
                
                if len(metric_dict) > 2:  # Only include if we have meaningful data
                    metrics_info.append(metric_dict)
            
            # Create the complete input structure
            secondary_input = {
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard_name,
                'original_dashboard_analysis': original_analysis,
                'dashboard_metrics': metrics_info,
                'actual_sql_results': actual_results,
                'data_availability': {
                    'has_primary_analysis': actual_results.get('primary_analysis_sql', {}).get('status') == 'success',
                    'has_structure_analysis': actual_results.get('structure_sql', {}).get('status') == 'success',
                    'has_validation_results': actual_results.get('validation_sql', {}).get('status') == 'success',
                    'has_business_rules': actual_results.get('business_rules_sql', {}).get('status') == 'success',
                    'metrics_count': len(metrics_info)
                }
            }
            
            # Get the secondary prompt
            secondary_prompt = design_secondary_analysis_prompt()
            
            # Format the prompt - this should work now with cleaned data
            formatted_prompt = secondary_prompt.format(
                dashboard_id=dashboard_id,
                dashboard_name=dashboard_name,
                original_dashboard_analysis=json.dumps(original_analysis, indent=2),
                actual_sql_results=json.dumps(actual_results, indent=2),
                primary_analysis_data=json.dumps(actual_results.get('primary_analysis_sql', {}), indent=2),
                structure_analysis_data=json.dumps(actual_results.get('structure_sql', {}), indent=2),
                validation_results=json.dumps(actual_results.get('validation_sql', {}), indent=2),
                business_rules_data=json.dumps(actual_results.get('business_rules_sql', {}), indent=2)
            )
            
            secondary_batch_data.append({
                'content': formatted_prompt,
                'dashboard_id': dashboard_id,
                'dashboard_name': dashboard_name,
                'data_quality': secondary_input['data_availability']
            })
            
            print(f"    âœ… Successfully prepared request")
            
        except Exception as e:
            print(f"    âš ï¸ Error preparing {dashboard_id}: {str(e)[:100]}")
            continue
    
    print(f"\nâœ… Successfully prepared {len(secondary_batch_data)} secondary analysis requests")
    
    # Show data quality summary
    if secondary_batch_data:
        print(f"\nðŸ“Š DATA QUALITY SUMMARY:")
        total_with_primary = sum(1 for req in secondary_batch_data if req['data_quality']['has_primary_analysis'])
        total_with_structure = sum(1 for req in secondary_batch_data if req['data_quality']['has_structure_analysis'])
        total_with_validation = sum(1 for req in secondary_batch_data if req['data_quality']['has_validation_results'])
        
        print(f"  Requests with primary analysis data: {total_with_primary}/{len(secondary_batch_data)}")
        print(f"  Requests with structure analysis data: {total_with_structure}/{len(secondary_batch_data)}")
        print(f"  Requests with validation data: {total_with_validation}/{len(secondary_batch_data)}")
        print(f"  Average metrics per dashboard: {sum(req['data_quality']['metrics_count'] for req in secondary_batch_data) / len(secondary_batch_data):.1f}")
    
    return secondary_batch_data



================================================================================
END OF FILE: functions.py
================================================================================


================================================================================
FILE: requirements.txt
SIZE: 491 characters
================================================================================

# Data manipulation and analysis
pandas>=2.0.0
numpy>=1.24.0

# Google Cloud services
google-cloud-bigquery>=3.11.0
google-cloud-storage>=2.10.0
google-cloud-aiplatform>=1.38.0

# Google authentication and protobuf
google-auth>=2.17.0
protobuf>=4.21.0

# Vertex AI (included in google-cloud-aiplatform but explicit for clarity)
# vertexai is included in google-cloud-aiplatform

# Additional Google Cloud dependencies (usually auto-installed)
google-api-core>=2.11.0
google-cloud-core>=2.3.0

================================================================================
END OF FILE: requirements.txt
================================================================================


================================================================================
FILE: sql/looker_sql.txt
SIZE: 4809 characters
================================================================================

WITH
  -- 1. Identify popular Looker Studio reports (with more than 3 distinct viewers and specific owners)
  LookerStudioPopularReports AS (
    SELECT
      assetId,
      assetTitle,
      assetOwner,
      REPLACE(REPLACE(assetId, 'https://lookerstudio.google.com/reporting/', ''), 'https://datastudio.google.com/reporting/', '') AS report_id_clean
    FROM (
      SELECT
        assetid,
        assettitle,
        assetOwner,
        COUNT(DISTINCT user) AS viewers
      FROM
        `ncau-data-newsquery-prd.cdm_looker_studio.looker_studio_audit_logs`
      GROUP BY
        assetid,
        assettitle,
        assetOwner
    )
    WHERE
      viewers >= 3
      AND assetOwner IN (INSERT) -- <--- Your list of asset owners here
  ),
  -- 2. Identify all unique users who have viewed any of these popular reports.
  UsersOfPopularReports AS (
    SELECT DISTINCT
      LOWER(user) AS username
    FROM
      `ncau-data-newsquery-prd.cdm_looker_studio.looker_studio_audit_logs` lsal
    INNER JOIN
      LookerStudioPopularReports lspr ON lsal.assetId = lspr.assetId
  ),
  -- 3. Get BigQuery audit logs for jobs initiated by the identified users,
  --    and extract the Looker Studio report ID from job labels.
  BigQueryAuditLogsFilteredByUser AS (
    SELECT
      protopayload_auditlog.authenticationInfo.principalEmail AS username,
      receiveTimestamp,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobName.jobId,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.startTime,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.endTime,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalSlotMs,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalProcessedBytes,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.totalBilledBytes,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobStatistics.reservation,
      (
        SELECT value
        FROM UNNEST(protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.labels)
        WHERE key = 'looker_studio_report_id'
      ) AS looker_studio_report_id,
      protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.query.query AS query_text
    FROM
      `ncau-data-newsquery-prd.ops_prd_bq_logs.cloudaudit_googleapis_com_data_access`
    WHERE
      DATE(receiveTimestamp) >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
      AND protopayload_auditlog.authenticationInfo.principalEmail IN (SELECT username FROM UsersOfPopularReports)
      AND (
            SELECT value
            FROM UNNEST(protopayload_auditlog.servicedata_v1_bigquery.jobCompletedEvent.job.jobConfiguration.labels)
            WHERE key = 'looker_studio_report_id'
          ) IS NOT NULL
  ),
  -- 4. Join BigQuery jobs with popular report info and rank jobs randomly per report
  RankedBigQueryJobsPerReport AS (
    SELECT
      bqa.jobId,
      bqa.username,
      lspr.assetTitle AS looker_studio_report_name,
      lspr.assetOwner,
      bqa.looker_studio_report_id,
      bqa.startTime,
      bqa.endTime,
      bqa.receiveTimestamp,
      DATE_DIFF(bqa.endTime, bqa.startTime, SECOND) AS runtime_seconds,
      bqa.totalSlotMs,
      bqa.totalProcessedBytes,
      bqa.totalBilledBytes,
      bqa.reservation,
      bqa.query_text,
      -- Use a random ordering for ROW_NUMBER to get 'different flavors'
      ROW_NUMBER() OVER (PARTITION BY bqa.looker_studio_report_id ORDER BY FARM_FINGERPRINT(bqa.jobId)) AS rn
      -- Or, for a time-based random sample (less truly random but still varies):
      -- ROW_NUMBER() OVER (PARTITION BY bqa.looker_studio_report_id ORDER BY RAND()) AS rn -- RAND() is non-deterministic
      -- Better for consistent random: ORDER BY FARM_FINGERPRINT(CONCAT(bqa.jobId, CAST(bqa.receiveTimestamp AS STRING)))
    FROM
      BigQueryAuditLogsFilteredByUser bqa
    INNER JOIN
      LookerStudioPopularReports lspr
    ON
      bqa.looker_studio_report_id = lspr.report_id_clean
  )
-- 5. Final Selection: Retrieve only the top N (e.g., 3) randomly sampled jobs for each popular report
SELECT
  jobId,
  username,
  looker_studio_report_name,
  assetOwner,
  looker_studio_report_id,
  startTime,
  endTime,
  runtime_seconds,
  totalSlotMs,
  totalProcessedBytes,
  totalBilledBytes,
  reservation,
  query_text
FROM
  RankedBigQueryJobsPerReport
WHERE
  rn <= 3 -- Select only the top 3 rows per report (randomly picked)
ORDER BY
  looker_studio_report_name,
  jobId; -- Order by report name and then jobId for consistent output

================================================================================
END OF FILE: sql/looker_sql.txt
================================================================================
